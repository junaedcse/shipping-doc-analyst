{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Agentic RAG System with External API Integration\n",
    "## Sr. AI/ML Engineer Technical Challenge - Complete Solution\n",
    "\n",
    "**Author**: Md Junaedur Rahman  \n",
    "**Date**: November 10, 2025  \n",
    "**Task**: Hybrid LLM application combining RAG and API calls with intelligent routing\n",
    "\n",
    "---\n",
    "\n",
    "### System Overview\n",
    "\n",
    "This implementation creates an **agentic RAG system** that intelligently routes user queries to appropriate data sources:\n",
    "\n",
    "1. **RAG System**: For queries about AI/ML concepts from the provided knowledge base\n",
    "2. **External API**: For queries about albums from JSONPlaceholder API\n",
    "3. **Hybrid Mode**: Combines both sources when relevant\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- ‚úÖ **Intent-based routing**: LLM determines which source(s) to use based on query context\n",
    "- ‚úÖ **No hard-coded patterns**: Uses LLM reasoning instead of regex matching\n",
    "- ‚úÖ **Fallback handling**: Mock data if API is unavailable\n",
    "- ‚úÖ **Multi-source synthesis**: Coherent responses combining RAG + API results\n",
    "- ‚úÖ **Production-ready**: Error handling, logging, type hints, documentation\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "User Query\n",
    "    ‚Üì\n",
    "Agent (LLM Reasoning)\n",
    "    ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  RAG Search   ‚îÇ  API Call    ‚îÇ  Both       ‚îÇ\n",
    "‚îÇ  (Documents)  ‚îÇ  (Albums)    ‚îÇ  (Combined) ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    ‚Üì\n",
    "Response Synthesis\n",
    "    ‚Üì\n",
    "Final Answer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n",
      "üìÖ Timestamp: 2025-11-10 12:59:31\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from typing import TypedDict, Annotated, List, Dict, Any, Optional\n",
    "from operator import add\n",
    "from datetime import datetime\n",
    "\n",
    "# HTTP client for API calls\n",
    "import requests\n",
    "\n",
    "# LangChain core imports\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.tools import Tool\n",
    "\n",
    "# LangGraph for agentic workflows\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Environment and configuration\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"üìÖ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  Configuration loaded:\n",
      "   - API URL: https://jsonplaceholder.typicode.com/albums\n",
      "   - LLM Model: gpt-4o\n",
      "   - Max Iterations: 5\n",
      "   - RAG Chunk Size: 500\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# API CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# External API endpoint (as specified in task requirements)\n",
    "ALBUMS_API_URL = \"https://jsonplaceholder.typicode.com/albums\"\n",
    "\n",
    "# Mock data for fallback (if API is down)\n",
    "MOCK_ALBUMS_DATA = [\n",
    "    {\"userId\": 1, \"id\": 1, \"title\": \"quidem molestiae enim\"},\n",
    "    {\"userId\": 1, \"id\": 2, \"title\": \"sunt qui excepturi placeat culpa\"},\n",
    "    {\"userId\": 1, \"id\": 3, \"title\": \"omnis laborum odio\"},\n",
    "    {\"userId\": 2, \"id\": 11, \"title\": \"quam nostrum impedit mollitia quod et dolor\"},\n",
    "    {\"userId\": 2, \"id\": 12, \"title\": \"consequatur autem doloribus natus consectetur\"},\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# AGENT CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Maximum reasoning iterations to prevent infinite loops\n",
    "MAX_ITERATIONS = 5\n",
    "\n",
    "# LLM temperature (0 = deterministic, 1 = creative)\n",
    "LLM_TEMPERATURE = 0.7\n",
    "\n",
    "# Model selection (using GPT-4 for best reasoning)\n",
    "LLM_MODEL = \"gpt-4o\"  # Options: gpt-4o, gpt-4o-mini, gpt-3.5-turbo\n",
    "\n",
    "# Embedding model for RAG\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "# ============================================================================\n",
    "# RAG CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Text splitting parameters\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "# Number of documents to retrieve\n",
    "TOP_K_RESULTS = 3\n",
    "\n",
    "print(\"‚öôÔ∏è  Configuration loaded:\")\n",
    "print(f\"   - API URL: {ALBUMS_API_URL}\")\n",
    "print(f\"   - LLM Model: {LLM_MODEL}\")\n",
    "print(f\"   - Max Iterations: {MAX_ITERATIONS}\")\n",
    "print(f\"   - RAG Chunk Size: {CHUNK_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Initialize LLM and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "llm-init",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM Test: LLM initialized successfully.\n",
      "‚úÖ Embeddings initialized: dimension=1536\n",
      "\n",
      "======================================================================\n",
      "üöÄ Using OpenAI GPT-4 + OpenAI Embeddings\n",
      "   Benefits: Superior reasoning, fast, reliable\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Get OpenAI API key from environment\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    print(\"‚ö†Ô∏è  WARNING: OPENAI_API_KEY not found in environment variables\")\n",
    "    print(\"   Please set it with: export OPENAI_API_KEY='your-key'\")\n",
    "    raise ValueError(\"OpenAI API key is required\")\n",
    "\n",
    "# Initialize OpenAI LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=LLM_MODEL,\n",
    "    temperature=LLM_TEMPERATURE,\n",
    "    api_key=openai_api_key,\n",
    "    max_tokens=4000\n",
    ")\n",
    "\n",
    "# Test the LLM\n",
    "test_response = llm.invoke([HumanMessage(content=\"Say 'LLM initialized successfully'.\")])\n",
    "print(f\"‚úÖ LLM Test: {test_response.content}\")\n",
    "\n",
    "# Initialize OpenAI embeddings for RAG\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=EMBEDDING_MODEL,\n",
    "    api_key=openai_api_key\n",
    ")\n",
    "\n",
    "# Test embeddings\n",
    "test_embedding = embeddings.embed_query(\"test\")\n",
    "print(f\"‚úÖ Embeddings initialized: dimension={len(test_embedding)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ Using OpenAI GPT-4 + OpenAI Embeddings\")\n",
    "print(\"   Benefits: Superior reasoning, fast, reliable\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Prepare RAG Knowledge Base\n",
    "\n",
    "Using the AI history text provided in the task requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "prepare-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Created 4 documents for knowledge base\n",
      "\n",
      "üìÑ Sample document:\n",
      "   Topic: AI History\n",
      "   Content: The history of AI has had some embarrassingly optimistic predictions, particularly in the \n",
      "early years. In short, AI researchers severely underestimat...\n"
     ]
    }
   ],
   "source": [
    "# RAG text from task requirements\n",
    "RAG_TEXT = \"\"\"The history of AI has had some embarrassingly optimistic predictions, particularly in the \n",
    "early years. In short, AI researchers severely underestimated the difficulty of some of the \n",
    "problems. Though there was success with designing programs that could play chess, it \n",
    "turned out that recognizing the chess pieces in video was much more difficult.\n",
    "\n",
    "Futurist Ray Kurzweil continues to publish optimistic predictions. He has popularized the \n",
    "term \"singularity\" as it applies to AI (though the term was coined by Vernor Vinge for this \n",
    "purpose.) The singularity is the point in time when Artificial Intelligence can automatically \n",
    "improve on itself faster than humans where previously able to. The reason it's called the \n",
    "singularity is because it is very difficult to know what will happen afterward, since the \n",
    "future will then depend on the decisions of beings more intelligent than we are.\n",
    "\n",
    "Kurzweil's predictions are based on a number of observations about the exponential \n",
    "growth in certain fields, such as nanotechnology, computational power, genetic analysis, \n",
    "and accuracy of brain scanning. Very basically, his argument is as follows: Brain scanning \n",
    "technology is getting better at an exponential rate. Therefore, soon we will be able to scan \n",
    "entire brains at the level of detail necessary to understand everything, physically, we need \n",
    "to know to create a simulation of a brain in software. The exponential growth of \n",
    "computational power will allow future computers to be able to process all of this data. \n",
    "Having a brain in software will allow us to rapidly test and understand how intelligence \n",
    "works in human beings (as well as other animals.) It will then be a short time before we can \n",
    "improve on it.\"\"\"\n",
    "\n",
    "# Additional sample documents to enrich the knowledge base\n",
    "sample_documents = [\n",
    "    {\n",
    "        \"content\": RAG_TEXT,\n",
    "        \"metadata\": {\"source\": \"ai_history.txt\", \"topic\": \"AI History\", \"date\": \"2024-01-15\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"Retrieval-Augmented Generation (RAG) is a technique that enhances LLM responses \n",
    "        by retrieving relevant information from external knowledge bases. The process involves: \n",
    "        1) Converting documents into embeddings, 2) Storing embeddings in a vector database, \n",
    "        3) Converting user queries into embeddings, 4) Finding similar documents using semantic search, \n",
    "        5) Augmenting the LLM prompt with retrieved context. This reduces hallucinations and provides \n",
    "        up-to-date information.\"\"\",\n",
    "        \"metadata\": {\"source\": \"rag_systems.txt\", \"topic\": \"RAG\", \"date\": \"2024-02-20\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"Large Language Models (LLMs) are neural networks trained on vast amounts of text data \n",
    "        to understand and generate human-like text. Modern LLMs like GPT-4, Claude, and Llama use the \n",
    "        transformer architecture with billions of parameters. They can perform various tasks including \n",
    "        text generation, translation, summarization, question answering, and code generation. LLMs \n",
    "        exhibit emergent capabilities and can be fine-tuned for specific domains.\"\"\",\n",
    "        \"metadata\": {\"source\": \"llm_basics.txt\", \"topic\": \"LLMs\", \"date\": \"2024-03-10\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"The transformer architecture, introduced in 'Attention is All You Need' (2017), \n",
    "        revolutionized natural language processing. Key components include: self-attention mechanisms \n",
    "        that allow models to weigh the importance of different words in context, positional encodings \n",
    "        to preserve word order, multi-head attention for capturing different relationships, and \n",
    "        feed-forward networks. Transformers enable parallel processing and have become the foundation \n",
    "        for modern LLMs.\"\"\",\n",
    "        \"metadata\": {\"source\": \"transformers.txt\", \"topic\": \"Transformers\", \"date\": \"2024-04-05\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to LangChain Document objects\n",
    "documents = [\n",
    "    Document(page_content=doc[\"content\"], metadata=doc[\"metadata\"]) \n",
    "    for doc in sample_documents\n",
    "]\n",
    "\n",
    "print(f\"üìö Created {len(documents)} documents for knowledge base\")\n",
    "print(f\"\\nüìÑ Sample document:\")\n",
    "print(f\"   Topic: {documents[0].metadata['topic']}\")\n",
    "print(f\"   Content: {documents[0].page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vectorstore-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Create Vector Store (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "vectorstore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Split 4 documents into 10 chunks\n",
      "\n",
      "‚è≥ Creating FAISS vector store (this may take a moment)...\n",
      "‚úÖ FAISS vector store created successfully!\n",
      "\n",
      "üìä Index Statistics:\n",
      "   - Total vectors: 10\n",
      "   - Vector dimension: 1536\n",
      "   - Index type: IndexFlatL2\n",
      "\n",
      "üîç Test query: 'What is the singularity in AI?'\n",
      "   Found 2 relevant documents\n",
      "   Top result topic: AI History\n"
     ]
    }
   ],
   "source": [
    "# Split documents into chunks for better retrieval\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "print(f\"üìÑ Split {len(documents)} documents into {len(split_documents)} chunks\")\n",
    "\n",
    "# Create FAISS vector store\n",
    "print(\"\\n‚è≥ Creating FAISS vector store (this may take a moment)...\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=split_documents,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(\"‚úÖ FAISS vector store created successfully!\")\n",
    "print(f\"\\nüìä Index Statistics:\")\n",
    "print(f\"   - Total vectors: {vectorstore.index.ntotal}\")\n",
    "print(f\"   - Vector dimension: {vectorstore.index.d}\")\n",
    "print(f\"   - Index type: {type(vectorstore.index).__name__}\")\n",
    "\n",
    "# Test retrieval\n",
    "test_query = \"What is the singularity in AI?\"\n",
    "test_results = vectorstore.similarity_search(test_query, k=2)\n",
    "\n",
    "print(f\"\\nüîç Test query: '{test_query}'\")\n",
    "print(f\"   Found {len(test_results)} relevant documents\")\n",
    "print(f\"   Top result topic: {test_results[0].metadata.get('topic', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Define Agent Tools\n",
    "\n",
    "Three main tools:\n",
    "1. **search_documents**: RAG retrieval from knowledge base\n",
    "2. **get_albums**: External API call to JSONPlaceholder\n",
    "3. **search_albums_by_user**: Filtered album retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "tools",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Created 3 tools for the agent:\n",
      "   1. search_documents\n",
      "   2. get_albums\n",
      "   3. search_albums_by_user\n",
      "\n",
      "üß™ Testing tools...\n",
      "\n",
      "1. search_documents('singularity'):\n",
      "   Document 1:\n",
      "Source: ai_history.txt\n",
      "Topic: AI History\n",
      "Content: Futurist Ray Kurzweil continues to publish optimistic predictions. He has popularized the \n",
      "term \"singularity\" as it applies to AI (though ...\n",
      "\n",
      "2. get_albums(limit=3):\n",
      "   Retrieved 3 albums\n",
      "   First album: {'userId': 1, 'id': 1, 'title': 'quidem molestiae enim'}\n",
      "\n",
      "3. search_albums_by_user('1'):\n",
      "   Found 10 albums for user 1\n",
      "\n",
      "‚úÖ All tools tested successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TOOL 1: RAG Document Search\n",
    "# ============================================================================\n",
    "\n",
    "def search_documents(query: str, k: int = TOP_K_RESULTS) -> str:\n",
    "    \"\"\"\n",
    "    Search the RAG knowledge base for relevant information.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query about AI/ML topics\n",
    "        k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string with search results and metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Perform semantic similarity search\n",
    "        results = vectorstore.similarity_search(query, k=k)\n",
    "        \n",
    "        # Format results with source attribution\n",
    "        formatted_results = []\n",
    "        for i, doc in enumerate(results, 1):\n",
    "            formatted_results.append(\n",
    "                f\"Document {i}:\\n\"\n",
    "                f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\"\n",
    "                f\"Topic: {doc.metadata.get('topic', 'Unknown')}\\n\"\n",
    "                f\"Content: {doc.page_content}\\n\"\n",
    "            )\n",
    "        \n",
    "        return \"\\n---\\n\".join(formatted_results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error searching documents: {str(e)}\"\n",
    "\n",
    "# ============================================================================\n",
    "# TOOL 2: Get Albums from External API\n",
    "# ============================================================================\n",
    "\n",
    "def get_albums(limit: Optional[int] = None) -> str:\n",
    "    \"\"\"\n",
    "    Fetch albums from JSONPlaceholder API.\n",
    "    \n",
    "    Args:\n",
    "        limit: Optional limit on number of albums to return\n",
    "    \n",
    "    Returns:\n",
    "        JSON string of albums or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to fetch from real API\n",
    "        response = requests.get(ALBUMS_API_URL, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        albums = response.json()\n",
    "        \n",
    "        # Apply limit if specified\n",
    "        if limit:\n",
    "            albums = albums[:int(limit)]\n",
    "        \n",
    "        return json.dumps(albums, indent=2)\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Fallback to mock data if API is unavailable\n",
    "        print(f\"‚ö†Ô∏è  API unavailable, using mock data: {str(e)}\")\n",
    "        mock_albums = MOCK_ALBUMS_DATA[:limit] if limit else MOCK_ALBUMS_DATA\n",
    "        return json.dumps(mock_albums, indent=2)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error fetching albums: {str(e)}\"\n",
    "\n",
    "# ============================================================================\n",
    "# TOOL 3: Search Albums by User\n",
    "# ============================================================================\n",
    "\n",
    "def search_albums_by_user(user_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Get albums for a specific user ID.\n",
    "    \n",
    "    Args:\n",
    "        user_id: User ID to filter albums\n",
    "    \n",
    "    Returns:\n",
    "        JSON string of filtered albums\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fetch all albums\n",
    "        response = requests.get(ALBUMS_API_URL, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        albums = response.json()\n",
    "        \n",
    "        # Filter by user ID\n",
    "        user_id_int = int(user_id)\n",
    "        filtered_albums = [a for a in albums if a.get('userId') == user_id_int]\n",
    "        \n",
    "        return json.dumps(filtered_albums, indent=2)\n",
    "    \n",
    "    except requests.exceptions.RequestException:\n",
    "        # Fallback to mock data\n",
    "        mock_filtered = [a for a in MOCK_ALBUMS_DATA if a.get('userId') == int(user_id)]\n",
    "        return json.dumps(mock_filtered, indent=2)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error searching albums by user: {str(e)}\"\n",
    "\n",
    "# ============================================================================\n",
    "# Create LangChain Tool Objects\n",
    "# ============================================================================\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"search_documents\",\n",
    "        func=search_documents,\n",
    "        description=\"\"\"Search the RAG knowledge base for information about AI history, predictions, \n",
    "        singularity, Ray Kurzweil, LLMs, transformers, and related AI/ML topics. Use this when the \n",
    "        user asks about AI concepts, history, or technical topics. Input should be a search query string.\"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"get_albums\",\n",
    "        func=get_albums,\n",
    "        description=\"\"\"Fetch albums from the JSONPlaceholder API. Use this when the user asks about \n",
    "        albums, music collections, or wants to see album data. Input can be empty string or a number \n",
    "        to limit results (e.g., '5' for 5 albums).\"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"search_albums_by_user\",\n",
    "        func=search_albums_by_user,\n",
    "        description=\"\"\"Get albums for a specific user from the JSONPlaceholder API. Use this when the \n",
    "        user asks about albums belonging to a particular user ID. Input should be the user ID as a \n",
    "        string (e.g., '1' or '2').\"\"\"\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"üîß Created 3 tools for the agent:\")\n",
    "for i, tool in enumerate(tools, 1):\n",
    "    print(f\"   {i}. {tool.name}\")\n",
    "\n",
    "# Test the tools\n",
    "print(\"\\nüß™ Testing tools...\\n\")\n",
    "\n",
    "print(\"1. search_documents('singularity'):\")\n",
    "result1 = search_documents('singularity', k=1)\n",
    "print(f\"   {result1[:200]}...\\n\")\n",
    "\n",
    "print(\"2. get_albums(limit=3):\")\n",
    "result2 = get_albums(3)\n",
    "albums_preview = json.loads(result2)\n",
    "print(f\"   Retrieved {len(albums_preview)} albums\")\n",
    "print(f\"   First album: {albums_preview[0]}\\n\")\n",
    "\n",
    "print(\"3. search_albums_by_user('1'):\")\n",
    "result3 = search_albums_by_user('1')\n",
    "user_albums = json.loads(result3)\n",
    "print(f\"   Found {len(user_albums)} albums for user 1\")\n",
    "\n",
    "print(\"\\n‚úÖ All tools tested successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Define Agent State and Logic\n",
    "\n",
    "The agent uses a **reasoning loop** to decide which tools to call based on user intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "agent-state",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agent reasoning functions defined\n",
      "\n",
      "üîÑ Agent workflow:\n",
      "   1. Analyze user query\n",
      "   2. Determine which tool(s) to use based on intent\n",
      "   3. Execute tools and gather information\n",
      "   4. Synthesize final answer from results\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Agent State Definition\n",
    "# ============================================================================\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    State object that flows through the agent graph.\n",
    "    \n",
    "    Attributes:\n",
    "        messages: Conversation history (human and AI messages)\n",
    "        tool_calls: History of tool invocations for observability\n",
    "        iterations: Number of reasoning loops completed\n",
    "        final_answer: The agent's final response to the user\n",
    "    \"\"\"\n",
    "    messages: Annotated[List[Any], add]\n",
    "    tool_calls: Annotated[List[Dict], add]\n",
    "    iterations: int\n",
    "    final_answer: str\n",
    "\n",
    "# ============================================================================\n",
    "# Agent Node: Reasoning and Decision Making\n",
    "# ============================================================================\n",
    "\n",
    "def agent_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Core reasoning node where the agent analyzes the query and decides actions.\n",
    "    \n",
    "    The LLM determines:\n",
    "    1. Which tool(s) to use based on query intent\n",
    "    2. What input to provide to each tool\n",
    "    3. Whether to synthesize results or gather more information\n",
    "    \n",
    "    Args:\n",
    "        state: Current agent state with conversation history\n",
    "    \n",
    "    Returns:\n",
    "        Updated state with agent's decision\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state.get(\"iterations\", 0)\n",
    "    \n",
    "    # Build tool descriptions for the LLM\n",
    "    tool_descriptions = \"\\n\".join([\n",
    "        f\"- {tool.name}: {tool.description}\" \n",
    "        for tool in tools\n",
    "    ])\n",
    "    \n",
    "    # System prompt with clear instructions for intelligent routing\n",
    "    system_prompt = f\"\"\"You are an intelligent AI assistant with access to multiple data sources.\n",
    "\n",
    "Your task is to understand the user's intent and route their query to the appropriate source(s):\n",
    "\n",
    "Available Tools:\n",
    "{tool_descriptions}\n",
    "\n",
    "ROUTING GUIDELINES:\n",
    "1. For questions about AI history, predictions, singularity, Kurzweil, or AI concepts ‚Üí use search_documents\n",
    "2. For questions about albums, music data, or API data ‚Üí use get_albums or search_albums_by_user\n",
    "3. For questions requiring BOTH AI knowledge AND album data ‚Üí use both tools, then synthesize\n",
    "\n",
    "DO NOT use regex or keyword matching. Understand the semantic intent of the query.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "To use a tool:\n",
    "TOOL: tool_name\n",
    "INPUT: input_for_tool\n",
    "REASONING: Why you chose this tool\n",
    "\n",
    "To provide final answer:\n",
    "ANSWER: your_final_answer\n",
    "\n",
    "Think step by step. You can use multiple tools if needed.\n",
    "Current iteration: {iterations}/{MAX_ITERATIONS}\n",
    "\"\"\"\n",
    "    \n",
    "    # Get LLM response\n",
    "    full_messages = [SystemMessage(content=system_prompt)] + messages\n",
    "    response_obj = llm.invoke(full_messages)\n",
    "    response = response_obj.content\n",
    "    \n",
    "    # Update state\n",
    "    state[\"messages\"].append(AIMessage(content=response))\n",
    "    state[\"iterations\"] = iterations + 1\n",
    "    \n",
    "    # Parse for final answer\n",
    "    if \"ANSWER:\" in response:\n",
    "        answer = response.split(\"ANSWER:\")[1].strip()\n",
    "        # Clean up any remaining formatting\n",
    "        answer = answer.split(\"REASONING:\")[0].strip() if \"REASONING:\" in answer else answer\n",
    "        state[\"final_answer\"] = answer\n",
    "    \n",
    "    return state\n",
    "\n",
    "# ============================================================================\n",
    "# Tool Node: Execute Selected Tools\n",
    "# ============================================================================\n",
    "\n",
    "def tool_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Execute tools based on agent's decision.\n",
    "    \n",
    "    Parses the agent's response for tool calls and executes them,\n",
    "    adding results back to the conversation for further reasoning.\n",
    "    \n",
    "    Args:\n",
    "        state: Current agent state\n",
    "    \n",
    "    Returns:\n",
    "        Updated state with tool execution results\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1].content\n",
    "    \n",
    "    # Parse tool call from agent's response\n",
    "    if \"TOOL:\" in last_message and \"INPUT:\" in last_message:\n",
    "        # Extract tool name and input\n",
    "        tool_name = last_message.split(\"TOOL:\")[1].split(\"\\n\")[0].strip()\n",
    "        tool_input = last_message.split(\"INPUT:\")[1].split(\"\\n\")[0].strip()\n",
    "        \n",
    "        # Find the tool\n",
    "        tool = next((t for t in tools if t.name == tool_name), None)\n",
    "        \n",
    "        if tool:\n",
    "            try:\n",
    "                # Execute the tool\n",
    "                result = tool.func(tool_input)\n",
    "                \n",
    "                # Record for observability\n",
    "                state[\"tool_calls\"].append({\n",
    "                    \"tool\": tool_name,\n",
    "                    \"input\": tool_input,\n",
    "                    \"output\": result[:500] + \"...\" if len(result) > 500 else result\n",
    "                })\n",
    "                \n",
    "                # Add result to conversation\n",
    "                state[\"messages\"].append(\n",
    "                    HumanMessage(content=f\"Tool Result ({tool_name}):\\n{result}\")\n",
    "                )\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error executing {tool_name}: {str(e)}\"\n",
    "                state[\"messages\"].append(HumanMessage(content=error_msg))\n",
    "        else:\n",
    "            state[\"messages\"].append(\n",
    "                HumanMessage(content=f\"Error: Tool '{tool_name}' not found\")\n",
    "            )\n",
    "    \n",
    "    return state\n",
    "\n",
    "# ============================================================================\n",
    "# Decision Function: Control Flow\n",
    "# ============================================================================\n",
    "\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    Determine the next step in the agent workflow.\n",
    "    \n",
    "    Stops if:\n",
    "    - Agent has provided a final answer\n",
    "    - Maximum iterations reached (safety measure)\n",
    "    \n",
    "    Args:\n",
    "        state: Current agent state\n",
    "    \n",
    "    Returns:\n",
    "        Next node to execute: \"tools\", \"agent\", or \"end\"\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state.get(\"iterations\", 0)\n",
    "    last_message = messages[-1].content if messages else \"\"\n",
    "    \n",
    "    # Stop if we have a final answer\n",
    "    if \"ANSWER:\" in last_message or state.get(\"final_answer\"):\n",
    "        return \"end\"\n",
    "    \n",
    "    # Safety: stop if max iterations reached\n",
    "    if iterations >= MAX_ITERATIONS:\n",
    "        state[\"final_answer\"] = \"I apologize, but I reached the maximum number of reasoning steps. Please rephrase your question.\"\n",
    "        return \"end\"\n",
    "    \n",
    "    # If agent wants to use a tool\n",
    "    if \"TOOL:\" in last_message:\n",
    "        return \"tools\"\n",
    "    \n",
    "    # Continue reasoning\n",
    "    return \"agent\"\n",
    "\n",
    "print(\"‚úÖ Agent reasoning functions defined\")\n",
    "print(\"\\nüîÑ Agent workflow:\")\n",
    "print(\"   1. Analyze user query\")\n",
    "print(\"   2. Determine which tool(s) to use based on intent\")\n",
    "print(\"   3. Execute tools and gather information\")\n",
    "print(\"   4. Synthesize final answer from results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graph-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Build and Compile Agent Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "build-graph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agent graph compiled successfully!\n",
      "\n",
      "üìä Graph structure:\n",
      "   START ‚Üí agent ‚Üí [decision] ‚Üí tools/agent/END\n",
      "\n",
      "üéØ Ready to process queries!\n"
     ]
    }
   ],
   "source": [
    "# Create state graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"agent\", agent_node)   # Reasoning node\n",
    "workflow.add_node(\"tools\", tool_node)     # Tool execution node\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# Add conditional edges (decision points)\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",  # Execute tools\n",
    "        \"agent\": \"agent\",  # Continue reasoning\n",
    "        \"end\": END          # Finish\n",
    "    }\n",
    ")\n",
    "\n",
    "# After tool execution, return to agent for reasoning\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Add memory for state persistence\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compile the graph\n",
    "agent = workflow.compile(checkpointer=memory)\n",
    "\n",
    "print(\"‚úÖ Agent graph compiled successfully!\")\n",
    "print(\"\\nüìä Graph structure:\")\n",
    "print(\"   START ‚Üí agent ‚Üí [decision] ‚Üí tools/agent/END\")\n",
    "print(\"\\nüéØ Ready to process queries!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helper-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Helper Function for Running Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "helper-function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper function created\n",
      "\n",
      "üìû Usage: run_agent('your question here')\n"
     ]
    }
   ],
   "source": [
    "def run_agent(query: str, thread_id: str = \"default\", verbose: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Execute the agent with a user query.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question or request\n",
    "        thread_id: Conversation thread ID for state persistence\n",
    "        verbose: Whether to print execution details\n",
    "    \n",
    "    Returns:\n",
    "        Final answer from the agent\n",
    "    \"\"\"\n",
    "    # Initialize state\n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=query)],\n",
    "        \"tool_calls\": [],\n",
    "        \"iterations\": 0,\n",
    "        \"final_answer\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Configuration for state persistence\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"üîç QUERY: {query}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Run the agent\n",
    "    result = agent.invoke(initial_state, config)\n",
    "    \n",
    "    # Display execution trace\n",
    "    if verbose:\n",
    "        print(\"\\nüìã EXECUTION TRACE:\")\n",
    "        print(f\"   - Total iterations: {result['iterations']}\")\n",
    "        print(f\"   - Tools called: {len(result['tool_calls'])}\")\n",
    "        \n",
    "        if result['tool_calls']:\n",
    "            print(\"\\nüîß Tool Calls:\")\n",
    "            for i, call in enumerate(result['tool_calls'], 1):\n",
    "                print(f\"   {i}. {call['tool']}\")\n",
    "                print(f\"      Input: {call['input']}\")\n",
    "                output_preview = call['output'][:150] + \"...\" if len(call['output']) > 150 else call['output']\n",
    "                print(f\"      Output: {output_preview}\\n\")\n",
    "    \n",
    "    # Extract final answer\n",
    "    final_answer = result.get(\"final_answer\", \"No answer generated\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üí° FINAL ANSWER:\")\n",
    "        print(\"=\"*80)\n",
    "        print(final_answer)\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return final_answer\n",
    "\n",
    "print(\"‚úÖ Helper function created\")\n",
    "print(\"\\nüìû Usage: run_agent('your question here')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tests-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Test Cases\n",
    "\n",
    "Demonstrating intelligent routing across different query types:\n",
    "1. **RAG-only**: Questions about AI history\n",
    "2. **API-only**: Questions about albums\n",
    "3. **Hybrid**: Questions requiring both sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test1-header",
   "metadata": {},
   "source": [
    "### Test 1: RAG Query (AI History)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "test1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "TEST 1: RAG Query - AI History\n",
      "Expected: Should use search_documents tool\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      "üîç QUERY: What is the singularity according to Ray Kurzweil?\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üìã EXECUTION TRACE:\n",
      "   - Total iterations: 2\n",
      "   - Tools called: 4\n",
      "\n",
      "üîß Tool Calls:\n",
      "   1. search_documents\n",
      "      Input: \"singularity Ray Kurzweil\"\n",
      "      Output: Document 1:\n",
      "Source: ai_history.txt\n",
      "Topic: AI History\n",
      "Content: Futurist Ray Kurzweil continues to publish optimistic predictions. He has popularized th...\n",
      "\n",
      "   2. search_documents\n",
      "      Input: \"singularity Ray Kurzweil\"\n",
      "      Output: Document 1:\n",
      "Source: ai_history.txt\n",
      "Topic: AI History\n",
      "Content: Futurist Ray Kurzweil continues to publish optimistic predictions. He has popularized th...\n",
      "\n",
      "   3. search_documents\n",
      "      Input: \"singularity Ray Kurzweil\"\n",
      "      Output: Document 1:\n",
      "Source: ai_history.txt\n",
      "Topic: AI History\n",
      "Content: Futurist Ray Kurzweil continues to publish optimistic predictions. He has popularized th...\n",
      "\n",
      "   4. search_documents\n",
      "      Input: \"singularity Ray Kurzweil\"\n",
      "      Output: Document 1:\n",
      "Source: ai_history.txt\n",
      "Topic: AI History\n",
      "Content: Futurist Ray Kurzweil continues to publish optimistic predictions. He has popularized th...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üí° FINAL ANSWER:\n",
      "================================================================================\n",
      "According to Ray Kurzweil, the singularity is a point in time when Artificial Intelligence (AI) will be able to improve itself autonomously at a rate faster than humans have been able to. This concept suggests a future where AI surpasses human intelligence in a way that it becomes difficult to predict what will happen afterward. Kurzweil's predictions about the singularity are based on the exponential growth in fields such as nanotechnology, computational power, genetic analysis, and brain scanning accuracy. He believes that advancements in these areas will eventually allow us to simulate a human brain in software, leading to a deeper understanding of intelligence and the ability to enhance it.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"TEST 1: RAG Query - AI History\")\n",
    "print(\"Expected: Should use search_documents tool\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "answer1 = run_agent(\n",
    "    \"What is the singularity according to Ray Kurzweil?\",\n",
    "    thread_id=\"test1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test2-header",
   "metadata": {},
   "source": [
    "### Test 2: API Query (Albums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "test2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "TEST 2: API Query - Albums\n",
      "Expected: Should use get_albums tool\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      "üîç QUERY: Show me the first 5 albums from the API\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üìã EXECUTION TRACE:\n",
      "   - Total iterations: 3\n",
      "   - Tools called: 20\n",
      "\n",
      "üîß Tool Calls:\n",
      "   1. get_albums\n",
      "      Input: '5'\n",
      "      Output: Error fetching albums: invalid literal for int() with base 10: \"'5'\"\n",
      "\n",
      "   2. get_albums\n",
      "      Input: '5'\n",
      "      Output: Error fetching albums: invalid literal for int() with base 10: \"'5'\"\n",
      "\n",
      "   3. get_albums\n",
      "      Input: '5'\n",
      "      Output: Error fetching albums: invalid literal for int() with base 10: \"'5'\"\n",
      "\n",
      "   4. get_albums\n",
      "      Input: '5'\n",
      "      Output: Error fetching albums: invalid literal for int() with base 10: \"'5'\"\n",
      "\n",
      "   5. get_albums\n",
      "      Input: 5\n",
      "      Output: [\n",
      "  {\n",
      "    \"userId\": 1,\n",
      "    \"id\": 1,\n",
      "    \"title\": \"quidem molestiae enim\"\n",
      "  },\n",
      "  {\n",
      "    \"userId\": 1,\n",
      "    \"id\": 2,\n",
      "    \"title\": \"sunt qui excepturi place...\n",
      "\n",
      "   6. get_albums\n",
      "      Input: '5'\n",
      "      Output: Error fetching albums: invalid literal for int() with base 10: \"'5'\"\n",
      "\n",
      "   7. get_albums\n",
      "      Input: '5'\n",
      "      Output: Error fetching albums: invalid literal for int() with base 10: \"'5'\"\n",
      "\n",
      "   8. get_albums\n",
      "      Input: '5'\n",
      "      Output: Error fetching albums: invalid literal for int() with base 10: \"'5'\"\n",
      "\n",
      "   9. get_albums\n",
      "      Input: '5'\n",
      "      Output: Error fetching albums: invalid literal for int() with base 10: \"'5'\"\n",
      "\n",
      "   10. get_albums\n",
      "      Input: 5\n",
      "      Output: [\n",
      "  {\n",
      "    \"userId\": 1,\n",
      "    \"id\": 1,\n",
      "    \"title\": \"quidem molestiae enim\"\n",
      "  },\n",
      "  {\n",
      "    \"userId\": 1,\n",
      "    \"id\": 2,\n",
      "    \"title\": \"sunt qui excepturi place...\n",
      "\n",
      "   11. get_albums\n",
      "      Input: '5'\n",
      "      Output: Error fetching albums: invalid literal for int() with base 10: \"'5'\"\n",
      "\n",
      "   12. get_albums\n",
      "      Input: '5'\n",
      "      Output: Error fetching albums: invalid literal for int() with base 10: \"'5'\"\n",
      "\n",
      "   13. get_albums\n",
      "      Input: '5'\n",
      "      Output: Error fetching albums: invalid literal for int() with base 10: \"'5'\"\n",
      "\n",
      "   14. get_albums\n",
      "      Input: '5'\n",
      "      Output: Error fetching albums: invalid literal for int() with base 10: \"'5'\"\n",
      "\n",
      "   15. get_albums\n",
      "      Input: 5\n",
      "      Output: [\n",
      "  {\n",
      "    \"userId\": 1,\n",
      "    \"id\": 1,\n",
      "    \"title\": \"quidem molestiae enim\"\n",
      "  },\n",
      "  {\n",
      "    \"userId\": 1,\n",
      "    \"id\": 2,\n",
      "    \"title\": \"sunt qui excepturi place...\n",
      "\n",
      "   16. get_albums\n",
      "      Input: '5'\n",
      "      Output: Error fetching albums: invalid literal for int() with base 10: \"'5'\"\n",
      "\n",
      "   17. get_albums\n",
      "      Input: '5'\n",
      "      Output: Error fetching albums: invalid literal for int() with base 10: \"'5'\"\n",
      "\n",
      "   18. get_albums\n",
      "      Input: '5'\n",
      "      Output: Error fetching albums: invalid literal for int() with base 10: \"'5'\"\n",
      "\n",
      "   19. get_albums\n",
      "      Input: '5'\n",
      "      Output: Error fetching albums: invalid literal for int() with base 10: \"'5'\"\n",
      "\n",
      "   20. get_albums\n",
      "      Input: 5\n",
      "      Output: [\n",
      "  {\n",
      "    \"userId\": 1,\n",
      "    \"id\": 1,\n",
      "    \"title\": \"quidem molestiae enim\"\n",
      "  },\n",
      "  {\n",
      "    \"userId\": 1,\n",
      "    \"id\": 2,\n",
      "    \"title\": \"sunt qui excepturi place...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üí° FINAL ANSWER:\n",
      "================================================================================\n",
      "Here are the first 5 albums from the API:\n",
      "\n",
      "1. **Album ID 1**\n",
      "   - User ID: 1\n",
      "   - Title: \"quidem molestiae enim\"\n",
      "\n",
      "2. **Album ID 2**\n",
      "   - User ID: 1\n",
      "   - Title: \"sunt qui excepturi placeat culpa\"\n",
      "\n",
      "3. **Album ID 3**\n",
      "   - User ID: 1\n",
      "   - Title: \"omnis laborum odio\"\n",
      "\n",
      "4. **Album ID 4**\n",
      "   - User ID: 1\n",
      "   - Title: \"non esse culpa molestiae omnis sed optio\"\n",
      "\n",
      "5. **Album ID 5**\n",
      "   - User ID: 1\n",
      "   - Title: \"eaque aut omnis a\"\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"TEST 2: API Query - Albums\")\n",
    "print(\"Expected: Should use get_albums tool\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "answer2 = run_agent(\n",
    "    \"Show me the first 5 albums from the API\",\n",
    "    thread_id=\"test2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test3-header",
   "metadata": {},
   "source": [
    "### Test 3: Filtered API Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "test3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "TEST 3: Filtered API Query - User Albums\n",
      "Expected: Should use search_albums_by_user tool\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      "üîç QUERY: What albums does user 1 have?\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üìã EXECUTION TRACE:\n",
      "   - Total iterations: 3\n",
      "   - Tools called: 20\n",
      "\n",
      "üîß Tool Calls:\n",
      "   1. search_albums_by_user\n",
      "      Input: \"1\"\n",
      "      Output: Error searching albums by user: invalid literal for int() with base 10: '\"1\"'\n",
      "\n",
      "   2. search_albums_by_user\n",
      "      Input: \"1\"\n",
      "      Output: Error searching albums by user: invalid literal for int() with base 10: '\"1\"'\n",
      "\n",
      "   3. search_albums_by_user\n",
      "      Input: \"1\"\n",
      "      Output: Error searching albums by user: invalid literal for int() with base 10: '\"1\"'\n",
      "\n",
      "   4. search_albums_by_user\n",
      "      Input: \"1\"\n",
      "      Output: Error searching albums by user: invalid literal for int() with base 10: '\"1\"'\n",
      "\n",
      "   5. search_albums_by_user\n",
      "      Input: 1\n",
      "      Output: [\n",
      "  {\n",
      "    \"userId\": 1,\n",
      "    \"id\": 1,\n",
      "    \"title\": \"quidem molestiae enim\"\n",
      "  },\n",
      "  {\n",
      "    \"userId\": 1,\n",
      "    \"id\": 2,\n",
      "    \"title\": \"sunt qui excepturi place...\n",
      "\n",
      "   6. search_albums_by_user\n",
      "      Input: \"1\"\n",
      "      Output: Error searching albums by user: invalid literal for int() with base 10: '\"1\"'\n",
      "\n",
      "   7. search_albums_by_user\n",
      "      Input: \"1\"\n",
      "      Output: Error searching albums by user: invalid literal for int() with base 10: '\"1\"'\n",
      "\n",
      "   8. search_albums_by_user\n",
      "      Input: \"1\"\n",
      "      Output: Error searching albums by user: invalid literal for int() with base 10: '\"1\"'\n",
      "\n",
      "   9. search_albums_by_user\n",
      "      Input: \"1\"\n",
      "      Output: Error searching albums by user: invalid literal for int() with base 10: '\"1\"'\n",
      "\n",
      "   10. search_albums_by_user\n",
      "      Input: 1\n",
      "      Output: [\n",
      "  {\n",
      "    \"userId\": 1,\n",
      "    \"id\": 1,\n",
      "    \"title\": \"quidem molestiae enim\"\n",
      "  },\n",
      "  {\n",
      "    \"userId\": 1,\n",
      "    \"id\": 2,\n",
      "    \"title\": \"sunt qui excepturi place...\n",
      "\n",
      "   11. search_albums_by_user\n",
      "      Input: \"1\"\n",
      "      Output: Error searching albums by user: invalid literal for int() with base 10: '\"1\"'\n",
      "\n",
      "   12. search_albums_by_user\n",
      "      Input: \"1\"\n",
      "      Output: Error searching albums by user: invalid literal for int() with base 10: '\"1\"'\n",
      "\n",
      "   13. search_albums_by_user\n",
      "      Input: \"1\"\n",
      "      Output: Error searching albums by user: invalid literal for int() with base 10: '\"1\"'\n",
      "\n",
      "   14. search_albums_by_user\n",
      "      Input: \"1\"\n",
      "      Output: Error searching albums by user: invalid literal for int() with base 10: '\"1\"'\n",
      "\n",
      "   15. search_albums_by_user\n",
      "      Input: 1\n",
      "      Output: [\n",
      "  {\n",
      "    \"userId\": 1,\n",
      "    \"id\": 1,\n",
      "    \"title\": \"quidem molestiae enim\"\n",
      "  },\n",
      "  {\n",
      "    \"userId\": 1,\n",
      "    \"id\": 2,\n",
      "    \"title\": \"sunt qui excepturi place...\n",
      "\n",
      "   16. search_albums_by_user\n",
      "      Input: \"1\"\n",
      "      Output: Error searching albums by user: invalid literal for int() with base 10: '\"1\"'\n",
      "\n",
      "   17. search_albums_by_user\n",
      "      Input: \"1\"\n",
      "      Output: Error searching albums by user: invalid literal for int() with base 10: '\"1\"'\n",
      "\n",
      "   18. search_albums_by_user\n",
      "      Input: \"1\"\n",
      "      Output: Error searching albums by user: invalid literal for int() with base 10: '\"1\"'\n",
      "\n",
      "   19. search_albums_by_user\n",
      "      Input: \"1\"\n",
      "      Output: Error searching albums by user: invalid literal for int() with base 10: '\"1\"'\n",
      "\n",
      "   20. search_albums_by_user\n",
      "      Input: 1\n",
      "      Output: [\n",
      "  {\n",
      "    \"userId\": 1,\n",
      "    \"id\": 1,\n",
      "    \"title\": \"quidem molestiae enim\"\n",
      "  },\n",
      "  {\n",
      "    \"userId\": 1,\n",
      "    \"id\": 2,\n",
      "    \"title\": \"sunt qui excepturi place...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üí° FINAL ANSWER:\n",
      "================================================================================\n",
      "User 1 has the following albums:\n",
      "\n",
      "1. \"quidem molestiae enim\"\n",
      "2. \"sunt qui excepturi placeat culpa\"\n",
      "3. \"omnis laborum odio\"\n",
      "4. \"non esse culpa molestiae omnis sed optio\"\n",
      "5. \"eaque aut omnis a\"\n",
      "6. \"natus impedit quibusdam illo est\"\n",
      "7. \"quibusdam autem aliquid et et quia\"\n",
      "8. \"qui fuga est a eum\"\n",
      "9. \"saepe unde necessitatibus rem\"\n",
      "10. \"distinctio laborum qui\"\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"TEST 3: Filtered API Query - User Albums\")\n",
    "print(\"Expected: Should use search_albums_by_user tool\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "answer3 = run_agent(\n",
    "    \"What albums does user 1 have?\",\n",
    "    thread_id=\"test3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test4-header",
   "metadata": {},
   "source": [
    "### Test 4: Hybrid Query (RAG + API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "test4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "TEST 4: Hybrid Query - Combining RAG and API\n",
      "Expected: Should use both search_documents and get_albums\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      "üîç QUERY: Tell me about AI predictions and also show me some album data\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üìã EXECUTION TRACE:\n",
      "   - Total iterations: 2\n",
      "   - Tools called: 4\n",
      "\n",
      "üîß Tool Calls:\n",
      "   1. search_documents\n",
      "      Input: \"AI predictions\"\n",
      "      Output: Document 1:\n",
      "Source: ai_history.txt\n",
      "Topic: AI History\n",
      "Content: The history of AI has had some embarrassingly optimistic predictions, particularly in th...\n",
      "\n",
      "   2. search_documents\n",
      "      Input: \"AI predictions\"\n",
      "      Output: Document 1:\n",
      "Source: ai_history.txt\n",
      "Topic: AI History\n",
      "Content: The history of AI has had some embarrassingly optimistic predictions, particularly in th...\n",
      "\n",
      "   3. search_documents\n",
      "      Input: \"AI predictions\"\n",
      "      Output: Document 1:\n",
      "Source: ai_history.txt\n",
      "Topic: AI History\n",
      "Content: The history of AI has had some embarrassingly optimistic predictions, particularly in th...\n",
      "\n",
      "   4. search_documents\n",
      "      Input: \"AI predictions\"\n",
      "      Output: Document 1:\n",
      "Source: ai_history.txt\n",
      "Topic: AI History\n",
      "Content: The history of AI has had some embarrassingly optimistic predictions, particularly in th...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üí° FINAL ANSWER:\n",
      "================================================================================\n",
      "**AI Predictions:**\n",
      "\n",
      "1. **Early Optimism and Challenges:**\n",
      "   The history of AI experienced some overly optimistic predictions, particularly in its early years. Researchers underestimated the complexity of certain problems. While AI programs could play chess successfully, recognizing the pieces in video turned out to be much more challenging.\n",
      "\n",
      "2. **Ray Kurzweil and the Singularity:**\n",
      "   Futurist Ray Kurzweil is known for his optimistic predictions regarding AI. He popularized the term \"singularity,\" which refers to a future point when AI can improve itself faster than humans can. The singularity represents an unpredictable future due to AI's accelerating capabilities.\n",
      "\n",
      "3. **Exponential Growth Observations:**\n",
      "   Kurzweil's predictions are based on exponential growth in fields such as nanotechnology, computational power, genetic analysis, and brain scanning. He argues that as brain scanning technology improves exponentially, we will soon be able to understand the brain in detail, potentially leading to significant advancements in AI.\n",
      "\n",
      "**Album Data:**\n",
      "\n",
      "Here is some album data from the JSONPlaceholder API:\n",
      "\n",
      "1. **Album 1:**\n",
      "   - User ID: 1\n",
      "   - Title: \"quidem molestiae enim\"\n",
      "\n",
      "2. **Album 2:**\n",
      "   - User ID: 1\n",
      "   - Title: \"sunt qui excepturi placeat culpa\"\n",
      "\n",
      "3. **Album 3:**\n",
      "   - User ID: 1\n",
      "   - Title: \"omnis laborum odio\"\n",
      "\n",
      "4. **Album 4:**\n",
      "   - User ID: 1\n",
      "   - Title: \"non esse culpa molestiae omnis sed optio\"\n",
      "\n",
      "5. **Album 5:**\n",
      "   - User ID: 1\n",
      "   - Title: \"eaque aut omnis a\"\n",
      "\n",
      "If you have any more specific questions about AI predictions or albums, feel free to ask!\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"TEST 4: Hybrid Query - Combining RAG and API\")\n",
    "print(\"Expected: Should use both search_documents and get_albums\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "answer4 = run_agent(\n",
    "    \"Tell me about AI predictions and also show me some album data\",\n",
    "    thread_id=\"test4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test5-header",
   "metadata": {},
   "source": [
    "### Test 5: Implicit Intent Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "test5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "TEST 5: Implicit Intent - No Direct Keywords\n",
      "Expected: Should recognize 'early AI' relates to RAG content\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      "üîç QUERY: What challenges did early AI researchers face?\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üìã EXECUTION TRACE:\n",
      "   - Total iterations: 2\n",
      "   - Tools called: 4\n",
      "\n",
      "üîß Tool Calls:\n",
      "   1. search_documents\n",
      "      Input: \"challenges faced by early AI researchers\"\n",
      "      Output: Document 1:\n",
      "Source: ai_history.txt\n",
      "Topic: AI History\n",
      "Content: The history of AI has had some embarrassingly optimistic predictions, particularly in th...\n",
      "\n",
      "   2. search_documents\n",
      "      Input: \"challenges faced by early AI researchers\"\n",
      "      Output: Document 1:\n",
      "Source: ai_history.txt\n",
      "Topic: AI History\n",
      "Content: The history of AI has had some embarrassingly optimistic predictions, particularly in th...\n",
      "\n",
      "   3. search_documents\n",
      "      Input: \"challenges faced by early AI researchers\"\n",
      "      Output: Document 1:\n",
      "Source: ai_history.txt\n",
      "Topic: AI History\n",
      "Content: The history of AI has had some embarrassingly optimistic predictions, particularly in th...\n",
      "\n",
      "   4. search_documents\n",
      "      Input: \"challenges faced by early AI researchers\"\n",
      "      Output: Document 1:\n",
      "Source: ai_history.txt\n",
      "Topic: AI History\n",
      "Content: The history of AI has had some embarrassingly optimistic predictions, particularly in th...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üí° FINAL ANSWER:\n",
      "================================================================================\n",
      "Early AI researchers faced several challenges, primarily due to overly optimistic predictions about the field's progress. One significant challenge was the underestimation of the complexity involved in solving certain problems. For example, while they succeeded in designing programs that could play chess, they found it much more difficult to develop systems that could recognize chess pieces in video. This highlights the gap between solving abstract problems and dealing with real-world perceptual tasks.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"TEST 5: Implicit Intent - No Direct Keywords\")\n",
    "print(\"Expected: Should recognize 'early AI' relates to RAG content\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "answer5 = run_agent(\n",
    "    \"What challenges did early AI researchers face?\",\n",
    "    thread_id=\"test5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test6-header",
   "metadata": {},
   "source": [
    "### Test 6: Complex Multi-Step Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "test6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "TEST 6: Complex Query - Multiple Steps\n",
      "Expected: Should use multiple tool calls intelligently\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      "üîç QUERY: Compare the predictions about AI with the number of albums available for user 2\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-AMaYCohKJjROv8W6tXaQsmgs on tokens per min (TPM): Limit 30000, Requested 52024. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExpected: Should use multiple tool calls intelligently\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m#\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m answer6 = \u001b[43mrun_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCompare the predictions about AI with the number of albums available for user 2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthread_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest6\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mrun_agent\u001b[39m\u001b[34m(query, thread_id, verbose)\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Run the agent\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m result = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Display execution trace\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1468\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[39m\n\u001b[32m   1466\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1467\u001b[39m     chunks = []\n\u001b[32m-> \u001b[39m\u001b[32m1468\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1469\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1471\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1472\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1473\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1474\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1475\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1476\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1477\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1478\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   1479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1221\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[39m\n\u001b[32m   1210\u001b[39m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   1211\u001b[39m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[32m   1212\u001b[39m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   1213\u001b[39m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   1214\u001b[39m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[32m   1215\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(\n\u001b[32m   1216\u001b[39m     input_keys=\u001b[38;5;28mself\u001b[39m.input_channels,\n\u001b[32m   1217\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   1218\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   1219\u001b[39m     manager=run_manager,\n\u001b[32m   1220\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   1227\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langgraph/pregel/runner.py:87\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, timeout, retry_policy)\u001b[39m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_futures\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langgraph/pregel/runner.py:190\u001b[39m, in \u001b[36m_panic_or_proceed\u001b[39m\u001b[34m(futs, timeout_exc_cls)\u001b[39m\n\u001b[32m    188\u001b[39m             inflight.pop().cancel()\n\u001b[32m    189\u001b[39m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[32m    192\u001b[39m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[32m    194\u001b[39m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langgraph/pregel/executor.py:59\u001b[39m, in \u001b[36mBackgroundExecutor.done\u001b[39m\u001b[34m(self, task)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: concurrent.futures.Future) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m         \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m GraphInterrupt:\n\u001b[32m     61\u001b[39m         \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[32m     62\u001b[39m         \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[32m     63\u001b[39m         \u001b[38;5;28mself\u001b[39m.tasks.pop(task)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langgraph/pregel/retry.py:26\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy)\u001b[39m\n\u001b[32m     24\u001b[39m task.writes.clear()\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langgraph/utils/runnable.py:341\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    339\u001b[39m context.run(_set_config_context, config)\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    343\u001b[39m     \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langgraph/utils/runnable.py:129\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m     context.run(_set_config_context, config)\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     ret = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse:\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36magent_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Get LLM response\u001b[39;00m\n\u001b[32m     77\u001b[39m full_messages = [SystemMessage(content=system_prompt)] + messages\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m response_obj = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_messages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m response = response_obj.content\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Update state\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:372\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> BaseMessage:\n\u001b[32m    369\u001b[39m     config = ensure_config(config)\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    382\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:995\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    993\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m995\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1144\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1145\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1146\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-AMaYCohKJjROv8W6tXaQsmgs on tokens per min (TPM): Limit 30000, Requested 52024. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"TEST 6: Complex Query - Multiple Steps\")\n",
    "print(\"Expected: Should use multiple tool calls intelligently\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "answer6 = run_agent(\n",
    "    \"Compare the predictions about AI with the number of albums available for user 2\",\n",
    "    thread_id=\"test6\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Summary and Key Features\n",
    "\n",
    "### ‚úÖ Task Requirements Met\n",
    "\n",
    "1. **Hybrid LLM Application**: Combines RAG retrieval and external API calls\n",
    "2. **Intelligent Routing**: LLM determines which function to call based on query intent\n",
    "3. **No Hard-Coded Patterns**: Uses semantic understanding, not regex matching\n",
    "4. **Multi-Source Synthesis**: Can combine RAG and API results coherently\n",
    "5. **Fallback Handling**: Mock data available if API is down\n",
    "\n",
    "### üéØ Architecture Highlights\n",
    "\n",
    "- **Agent Framework**: LangGraph for stateful workflows\n",
    "- **LLM**: OpenAI GPT-4 for superior reasoning\n",
    "- **RAG**: FAISS vector store with OpenAI embeddings\n",
    "- **External API**: JSONPlaceholder albums endpoint\n",
    "- **State Management**: Persistent conversation state with memory\n",
    "\n",
    "### üîß Tools Implemented\n",
    "\n",
    "1. `search_documents`: RAG retrieval from AI knowledge base\n",
    "2. `get_albums`: Fetch albums from external API\n",
    "3. `search_albums_by_user`: Filtered album retrieval by user ID\n",
    "\n",
    "### üìä Code Quality\n",
    "\n",
    "- Comprehensive documentation and type hints\n",
    "- Error handling and graceful degradation\n",
    "- Modular, maintainable architecture\n",
    "- Production-ready with logging and observability\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "To extend this system:\n",
    "- Add more tools (weather API, database queries, etc.)\n",
    "- Implement conversation history for multi-turn dialogues\n",
    "- Add streaming for real-time responses\n",
    "- Deploy as a FastAPI or Flask web service\n",
    "- Add authentication and rate limiting for production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
