{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Agentic RAG System with LangChain, LangGraph, FAISS & MCP\n",
    "\n",
    "## Overview\n",
    "This notebook implements a complete agentic RAG (Retrieval-Augmented Generation) system with flexible LLM options.\n",
    "\n",
    "### Components:\n",
    "1. **FAISS Vector Store**: Fast, efficient vector database for semantic search\n",
    "2. **LangChain**: Framework for LLM applications\n",
    "3. **LangGraph**: For agentic workflows with state management\n",
    "4. **LLM Options**: OpenAI GPT-4 or Local Ollama (Llama 3)\n",
    "5. **MCP (Model Context Protocol)**: For tool integration\n",
    "6. **Embeddings**: OpenAI or HuggingFace (local)\n",
    "7. **PyMuPDF**: Fast, reliable PDF processing\n",
    "\n",
    "### What This System Does:\n",
    "- Creates an autonomous agent that can:\n",
    "  - Search documents using semantic similarity\n",
    "  - Reason about retrieved information\n",
    "  - Answer questions with citations\n",
    "  - Use tools via MCP protocol\n",
    "  - Maintain conversation state\n",
    "\n",
    "\n",
    "### Why FAISS + PyMuPDF?\n",
    "- **FAISS**: Facebook's industry-standard vector database - fast, memory-efficient, scalable\n",
    "- **PyMuPDF (fitz)**: 3-5x faster than pypdf, better text extraction, handles complex PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Installation and Imports\n",
    "\n",
    "Installing all required packages. Works with both OpenAI and local Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All packages installed successfully!\n",
      "\n",
      " Key packages:\n",
      "  - FAISS: Vector database for semantic search\n",
      "  - PyMuPDF: Fast and reliable PDF processing\n",
      "  - LangChain + LangGraph: Agentic framework\n",
      "  - OpenAI + Ollama: Flexible LLM options\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# Run this cell first to install all dependencies\n",
    "\n",
    "!pip install -q langchain langchain-community langchain-core\n",
    "!pip install -q langchain-openai  # For OpenAI models\n",
    "!pip install -q langgraph\n",
    "!pip install -q faiss-cpu  # FAISS vector database (CPU version)\n",
    "!pip install -q sentence-transformers  # For local embeddings\n",
    "!pip install -q ollama  # Python client for Ollama (optional if using OpenAI)\n",
    "!pip install -q openai  # OpenAI Python client\n",
    "!pip install -q tiktoken  # For OpenAI token counting\n",
    "!pip install -q pymupdf  # For PDF document loading (better than pypdf)\n",
    "!pip install -q beautifulsoup4  # For HTML parsing\n",
    "!pip install -q mcp  # Model Context Protocol\n",
    "\n",
    "print(\" All packages installed successfully!\")\n",
    "print(\"\\n Key packages:\")\n",
    "print(\"  - FAISS: Vector database for semantic search\")\n",
    "print(\"  - PyMuPDF: Fast and reliable PDF processing\")\n",
    "print(\"  - LangChain + LangGraph: Agentic framework\")\n",
    "print(\"  - OpenAI + Ollama: Flexible LLM options\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "import os\n",
    "import json\n",
    "from typing import TypedDict, Annotated, List, Dict, Any\n",
    "from operator import add\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.tools import Tool\n",
    "\n",
    "# LangGraph imports for agentic workflows\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolExecutor, ToolInvocation\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Standard library imports\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\" All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize LLM and Embeddings\n",
    "\n",
    "**Choose your LLM provider:**\n",
    "- OpenAI (GPT-4, GPT-3.5) - Best quality, requires API key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing OpenAI models...\n",
      "\n",
      " LLM Test Response: Hello, I am using OpenAI!\n",
      "\n",
      "Initializing OpenAI embeddings...\n",
      " Embedding dimension: 1536\n",
      " First 5 values: [0.024632183834910393, -1.6649944882374257e-05, -0.013136658817529678, -0.007176156155765057, -0.02695712260901928]\n",
      "\n",
      " OpenAI LLM and embeddings initialized successfully!\n",
      "\n",
      "============================================================\n",
      " Using OpenAI GPT-4 + OpenAI Embeddings\n",
      "   Benefits: Best quality, fast, reliable\n",
      "   Cost: Pay per token usage\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CONFIGURATION: Choose the LLM provider\n",
    "# ========================================\n",
    "\n",
    "\n",
    "\n",
    "print(\"Initializing OpenAI models...\")\n",
    "\n",
    "# Get API key from environment variable\n",
    "# Make sure you've set: export OPENAI_API_KEY='your-key'\n",
    "# Or set it directly here (not recommended for production)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    print(\"  Warning: OPENAI_API_KEY not found in environment variables\")\n",
    "    print(\"Please set it with: export OPENAI_API_KEY='your-key'\")\n",
    "    print(\"Or uncomment the line below and add your key directly:\")\n",
    "    # openai_api_key = \"sk-...\"  # Uncomment and add your key\n",
    "\n",
    "# Initialize OpenAI LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",  # Options: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo\n",
    "    temperature=0.7,        # Controls randomness (0=deterministic, 1=creative)\n",
    "    api_key=openai_api_key,\n",
    "    max_tokens=2000         # Maximum response length\n",
    ")\n",
    "\n",
    "# Test the LLM\n",
    "test_response = llm.invoke([HumanMessage(content=\"Say 'Hello, I am using OpenAI!'\")])\n",
    "print(f\"\\n LLM Test Response: {test_response.content}\\n\")\n",
    "\n",
    "# Initialize OpenAI embeddings (faster and higher quality)\n",
    "print(\"Initializing OpenAI embeddings...\")\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # Options: text-embedding-3-small, text-embedding-3-large\n",
    "    api_key=openai_api_key\n",
    ")\n",
    "\n",
    "# Test embeddings\n",
    "test_embedding = embeddings.embed_query(\"This is a test sentence.\")\n",
    "print(f\" Embedding dimension: {len(test_embedding)}\")\n",
    "print(f\" First 5 values: {test_embedding[:5]}\")\n",
    "print(\"\\n OpenAI LLM and embeddings initialized successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Summary\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "print(\" Using OpenAI GPT-4 + OpenAI Embeddings\")\n",
    "print(\"   Benefits: Best quality, fast, reliable\")\n",
    "print(\"   Cost: Pay per token usage\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Sample Knowledge Base\n",
    "\n",
    "We'll create sample documents about AI and ML topics for our RAG system.\n",
    "In production, you would load documents from PDFs using PyMuPDF, websites, or databases.\n",
    "\n",
    "**PyMuPDF Benefits:**\n",
    "- 3-5x faster than pypdf\n",
    "- Better text extraction quality\n",
    "- Handles complex PDFs (forms, tables, images)\n",
    "- Reliable with large documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Created 7 sample documents\n",
      "\n",
      " Sample document preview:\n",
      "Content: Artificial Intelligence (AI) agents are autonomous systems that perceive their \n",
      "        environment, make decisions, and take actions to achieve speci...\n",
      "Metadata: {'source': 'ai_fundamentals.txt', 'topic': 'AI Agents', 'date': '2024-01-15'}\n"
     ]
    }
   ],
   "source": [
    "# Create sample documents for our knowledge base\n",
    "# These simulate real documents that would be loaded from files\n",
    "\n",
    "sample_documents = [\n",
    "    {\n",
    "        \"content\": \"\"\"Artificial Intelligence (AI) agents are autonomous systems that perceive their \n",
    "        environment, make decisions, and take actions to achieve specific goals. Modern AI agents \n",
    "        use Large Language Models (LLMs) as their reasoning engine, combined with tools for \n",
    "        external actions. Key characteristics include autonomy, goal-directed behavior, perception, \n",
    "        action capability, and reasoning.\"\"\",\n",
    "        \"metadata\": {\"source\": \"ai_fundamentals.txt\", \"topic\": \"AI Agents\", \"date\": \"2024-01-15\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"Retrieval-Augmented Generation (RAG) is a technique that enhances LLM responses \n",
    "        by retrieving relevant information from external knowledge bases. The process involves: \n",
    "        1) Converting documents into embeddings, 2) Storing embeddings in a vector database, \n",
    "        3) Converting user queries into embeddings, 4) Finding similar documents using semantic search, \n",
    "        5) Augmenting the LLM prompt with retrieved context. This reduces hallucinations and provides \n",
    "        up-to-date information.\"\"\",\n",
    "        \"metadata\": {\"source\": \"rag_systems.txt\", \"topic\": \"RAG\", \"date\": \"2024-02-20\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"FAISS (Facebook AI Similarity Search) is a library for efficient similarity search \n",
    "        and clustering of dense vectors. It's optimized for large-scale vector search and supports \n",
    "        various index types including flat (exact search), IVF (inverted file index), and HNSW \n",
    "        (Hierarchical Navigable Small World). FAISS is particularly useful for RAG systems where \n",
    "        fast retrieval of similar documents is crucial.\"\"\",\n",
    "        \"metadata\": {\"source\": \"vector_databases.txt\", \"topic\": \"FAISS\", \"date\": \"2024-03-10\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"LangGraph is a framework for building stateful, multi-actor applications with LLMs. \n",
    "        It extends LangChain by providing graph-based orchestration where nodes represent computation \n",
    "        steps and edges represent transitions. Key features include: state management across nodes, \n",
    "        conditional branching, cycles for iterative processing, human-in-the-loop patterns, and \n",
    "        checkpointing for persistence. LangGraph is ideal for complex agentic workflows.\"\"\",\n",
    "        \"metadata\": {\"source\": \"langgraph_guide.txt\", \"topic\": \"LangGraph\", \"date\": \"2024-04-05\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"The Model Context Protocol (MCP) is an open standard for connecting AI agents \n",
    "        with external tools and data sources. MCP servers expose capabilities through a standardized \n",
    "        interface including: tools (functions the agent can call), resources (data the agent can access), \n",
    "        and prompts (reusable templates). This allows agents to interact with databases, APIs, file systems, \n",
    "        and other services in a consistent way.\"\"\",\n",
    "        \"metadata\": {\"source\": \"mcp_protocol.txt\", \"topic\": \"MCP\", \"date\": \"2024-05-12\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"Embeddings are dense vector representations of text that capture semantic meaning. \n",
    "        Similar texts have similar embeddings (measured by cosine similarity or dot product). \n",
    "        Popular embedding models include OpenAI's text-embedding-ada-002, sentence-transformers \n",
    "        from HuggingFace, and specialized models like instructor embeddings. Good embeddings are \n",
    "        crucial for RAG system performance as they determine retrieval quality.\"\"\",\n",
    "        \"metadata\": {\"source\": \"embeddings_explained.txt\", \"topic\": \"Embeddings\", \"date\": \"2024-06-18\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"Tool use in AI agents involves the ability to call external functions or APIs to \n",
    "        extend capabilities beyond text generation. The process typically involves: 1) Tool description \n",
    "        (schema defining inputs/outputs), 2) Tool selection (agent decides which tool to use), \n",
    "        3) Parameter extraction (agent generates proper arguments), 4) Execution (framework calls the tool), \n",
    "        5) Result integration (agent processes the output). This enables agents to perform calculations, \n",
    "        search the web, query databases, and more.\"\"\",\n",
    "        \"metadata\": {\"source\": \"tool_usage.txt\", \"topic\": \"Tools\", \"date\": \"2024-07-22\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to LangChain Document objects\n",
    "documents = [\n",
    "    Document(page_content=doc[\"content\"], metadata=doc[\"metadata\"]) \n",
    "    for doc in sample_documents\n",
    "]\n",
    "\n",
    "print(f\" Created {len(documents)} sample documents\")\n",
    "print(f\"\\n Sample document preview:\")\n",
    "print(f\"Content: {documents[0].page_content[:150]}...\")\n",
    "print(f\"Metadata: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " To load your own PDFs:\n",
      "  1. Place your PDF in the same directory\n",
      "  2. Uncomment: documents = load_pdf_with_pymupdf('your_file.pdf')\n",
      "  3. Run this cell to load the PDF\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# BONUS: How to Load PDFs with PyMuPDF\n",
    "# ==========================================\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def load_pdf_with_pymupdf(pdf_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load PDF using PyMuPDF (faster and better than pypdf).\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "    \n",
    "    Returns:\n",
    "        List of Document objects with text and metadata\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    try:\n",
    "        # Open PDF\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        \n",
    "        # Extract text from each page\n",
    "        for page_num in range(len(pdf_document)):\n",
    "            page = pdf_document[page_num]\n",
    "            text = page.get_text()\n",
    "            \n",
    "            # Skip empty pages\n",
    "            if text.strip():\n",
    "                # Create document with metadata\n",
    "                doc = Document(\n",
    "                    page_content=text,\n",
    "                    metadata={\n",
    "                        \"source\": pdf_path,\n",
    "                        \"page\": page_num + 1,\n",
    "                        \"total_pages\": len(pdf_document)\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "        \n",
    "        pdf_document.close()\n",
    "        print(f\" Loaded {len(documents)} pages from {pdf_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error loading PDF: {str(e)}\")\n",
    "        return []\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Example usage (uncomment to use):\n",
    "# documents = load_pdf_with_pymupdf(\"your_document.pdf\")\n",
    "\n",
    "print(\"\\n To load your own PDFs:\")\n",
    "print(\"  1. Place your PDF in the same directory\")\n",
    "print(\"  2. Uncomment: documents = load_pdf_with_pymupdf('your_file.pdf')\")\n",
    "print(\"  3. Run this cell to load the PDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Text Splitting and Chunking\n",
    "\n",
    "Split documents into smaller chunks for better retrieval.\n",
    "Smaller chunks = more precise retrieval but might lose context.\n",
    "Larger chunks = more context but less precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Split 7 documents into 9 chunks\n",
      "\n",
      " Chunk statistics:\n",
      "  - Average chunk size: 362 characters\n",
      "  - Min chunk size: 23 characters\n",
      "  - Max chunk size: 497 characters\n",
      "\n",
      " Sample chunk:\n",
      "Content: Artificial Intelligence (AI) agents are autonomous systems that perceive their \n",
      "        environment, make decisions, and take actions to achieve specific goals. Modern AI agents \n",
      "        use Large Language Models (LLMs) as their reasoning engine, combined with tools for \n",
      "        external actions. Key characteristics include autonomy, goal-directed behavior, perception, \n",
      "        action capability, and reasoning.\n",
      "Metadata: {'source': 'ai_fundamentals.txt', 'topic': 'AI Agents', 'date': '2024-01-15'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize text splitter\n",
    "# RecursiveCharacterTextSplitter tries to keep semantically related text together\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,        # Maximum characters per chunk\n",
    "    chunk_overlap=50,      # Overlap between chunks to maintain context\n",
    "    length_function=len,   # Function to measure chunk size\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Try these separators in order\n",
    ")\n",
    "\n",
    "# Split documents into chunks\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\" Split {len(documents)} documents into {len(split_documents)} chunks\")\n",
    "print(f\"\\n Chunk statistics:\")\n",
    "chunk_lengths = [len(doc.page_content) for doc in split_documents]\n",
    "print(f\"  - Average chunk size: {sum(chunk_lengths) / len(chunk_lengths):.0f} characters\")\n",
    "print(f\"  - Min chunk size: {min(chunk_lengths)} characters\")\n",
    "print(f\"  - Max chunk size: {max(chunk_lengths)} characters\")\n",
    "\n",
    "# Show a sample chunk\n",
    "print(f\"\\n Sample chunk:\")\n",
    "print(f\"Content: {split_documents[0].page_content}\")\n",
    "print(f\"Metadata: {split_documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 5: Create FAISS Vector Store\n",
    "\n",
    "Build the vector database for semantic search using FAISS (Facebook AI Similarity Search).\n",
    "\n",
    "### Why FAISS?\n",
    "- **Speed**: 10-100x faster than basic vector search\n",
    "- **Memory Efficient**: Optimized for large-scale datasets\n",
    "- **Industry Standard**: Used by Facebook, Google, Microsoft\n",
    "- **Scalable**: Handles millions of vectors efficiently\n",
    "- **Flexible**: Multiple index types (Flat, IVF, HNSW) for different use cases\n",
    "\n",
    "FAISS indexes document embeddings for lightning-fast similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS vector store...\n",
      "(This may take a minute for embedding generation)\n",
      "\n",
      " FAISS vector store created successfully!\n",
      "\n",
      " Index Statistics:\n",
      "  - Total vectors indexed: 9\n",
      "  - Vector dimension: 1536\n",
      "  - Index type: IndexFlatL2\n",
      "\n",
      " Test query: 'What is RAG?'\n",
      "\n",
      " Top 2 results:\n",
      "\n",
      "1. Source: rag_systems.txt\n",
      "   Topic: RAG\n",
      "   Content preview: Retrieval-Augmented Generation (RAG) is a technique that enhances LLM responses \n",
      "        by retrieving relevant information from external knowledge ba...\n",
      "\n",
      "2. Source: vector_databases.txt\n",
      "   Topic: FAISS\n",
      "   Content preview: FAISS (Facebook AI Similarity Search) is a library for efficient similarity search \n",
      "        and clustering of dense vectors. It's optimized for large-...\n",
      "\n",
      " Vector store saved to './faiss_index'\n",
      "\n",
      " FAISS Performance Tips:\n",
      "  - Flat index (default): Best for <1M vectors, exact search\n",
      "  - IVF index: Better for 1M-10M vectors, approximate search\n",
      "  - HNSW index: Best for 10M+ vectors, fastest approximate search\n",
      "  - Current setup uses Flat index (exact, reliable)\n"
     ]
    }
   ],
   "source": [
    "# Create FAISS vector store from documents\n",
    "# This will:\n",
    "# 1. Generate embeddings for all document chunks\n",
    "# 2. Build a FAISS index for fast similarity search\n",
    "# 3. Store document metadata for retrieval\n",
    "\n",
    "print(\"Creating FAISS vector store...\")\n",
    "print(\"(This may take a minute for embedding generation)\\n\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=split_documents,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(\" FAISS vector store created successfully!\")\n",
    "print(f\"\\n Index Statistics:\")\n",
    "print(f\"  - Total vectors indexed: {vectorstore.index.ntotal}\")\n",
    "print(f\"  - Vector dimension: {vectorstore.index.d}\")\n",
    "print(f\"  - Index type: {type(vectorstore.index).__name__}\")\n",
    "\n",
    "# Test the vector store with a sample query\n",
    "test_query = \"What is RAG?\"\n",
    "print(f\"\\n Test query: '{test_query}'\")\n",
    "\n",
    "# Perform similarity search\n",
    "test_results = vectorstore.similarity_search(\n",
    "    query=test_query,\n",
    "    k=2  # Return top 2 most similar documents\n",
    ")\n",
    "\n",
    "print(f\"\\n Top {len(test_results)} results:\")\n",
    "for i, doc in enumerate(test_results, 1):\n",
    "    print(f\"\\n{i}. Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"   Topic: {doc.metadata.get('topic', 'Unknown')}\")\n",
    "    print(f\"   Content preview: {doc.page_content[:150]}...\")\n",
    "\n",
    "# Save the vector store locally (optional but recommended)\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "print(\"\\n Vector store saved to './faiss_index'\")\n",
    "\n",
    "print(\"\\n FAISS Performance Tips:\")\n",
    "print(\"  - Flat index (default): Best for <1M vectors, exact search\")\n",
    "print(\"  - IVF index: Better for 1M-10M vectors, approximate search\")\n",
    "print(\"  - HNSW index: Best for 10M+ vectors, fastest approximate search\")\n",
    "print(\"  - Current setup uses Flat index (exact, reliable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create RAG Tools for the Agent\n",
    "\n",
    "Define tools that our agent can use:\n",
    "1. **search_documents**: Semantic search in the vector store\n",
    "2. **get_current_time**: Get current timestamp\n",
    "3. **calculator**: Perform basic calculations\n",
    "\n",
    "These tools follow the MCP-inspired pattern for tool definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Created 3 tools for the agent:\n",
      "  - search_documents: Search the document knowledge base for information about AI, ML, RAG systems, \n",
      " ...\n",
      "  - get_current_time: Get the current date and time. Use this when asked about the current time or dat...\n",
      "  - calculator: Perform mathematical calculations. Input should be a mathematical expression \n",
      "  ...\n",
      "\n",
      " Testing tools:\n",
      "\n",
      "1. search_documents('embeddings'):\n",
      "Result 1:\n",
      "Source: embeddings_explained.txt\n",
      "Topic: Embeddings\n",
      "Content: Embeddings are dense vector representations of text that capture semantic meaning. \n",
      "        Similar texts have similar embeddings ...\n",
      "\n",
      "2. get_current_time():\n",
      "2025-11-10 12:00:05\n",
      "\n",
      "3. calculator('2 + 2'):\n",
      "Result: 4\n"
     ]
    }
   ],
   "source": [
    "# Define the document search tool\n",
    "def search_documents(query: str, k: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Search the document knowledge base for relevant information.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        k: Number of results to return (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string with search results and metadata\n",
    "    \"\"\"\n",
    "    # Perform similarity search\n",
    "    results = vectorstore.similarity_search(query, k=k)\n",
    "    \n",
    "    # Format results with source attribution\n",
    "    formatted_results = []\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        formatted_results.append(\n",
    "            f\"Result {i}:\\n\"\n",
    "            f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\"\n",
    "            f\"Topic: {doc.metadata.get('topic', 'Unknown')}\\n\"\n",
    "            f\"Content: {doc.page_content}\\n\"\n",
    "        )\n",
    "    \n",
    "    return \"\\n---\\n\".join(formatted_results)\n",
    "\n",
    "# Define a time tool\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"\n",
    "    Get the current date and time.\n",
    "    \n",
    "    Returns:\n",
    "        Current timestamp as a formatted string\n",
    "    \"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Define a simple calculator tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    Evaluate a mathematical expression.\n",
    "    \n",
    "    Args:\n",
    "        expression: Mathematical expression to evaluate (e.g., \"2 + 2\")\n",
    "    \n",
    "    Returns:\n",
    "        Result of the calculation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Safe evaluation of basic math expressions\n",
    "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
    "        return f\"Result: {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Wrap functions as LangChain Tools\n",
    "# Tools need a name, description, and function\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"search_documents\",\n",
    "        func=search_documents,\n",
    "        description=\"\"\"Search the document knowledge base for information about AI, ML, RAG systems, \n",
    "        LangGraph, FAISS, embeddings, and related topics. Use this when you need to find specific \n",
    "        information from the knowledge base. Input should be a search query string.\"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"get_current_time\",\n",
    "        func=get_current_time,\n",
    "        description=\"Get the current date and time. Use this when asked about the current time or date.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"calculator\",\n",
    "        func=calculator,\n",
    "        description=\"\"\"Perform mathematical calculations. Input should be a mathematical expression \n",
    "        as a string (e.g., '2 + 2' or '10 * 5'). Supports basic arithmetic operations.\"\"\"\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\" Created 3 tools for the agent:\")\n",
    "for tool in tools:\n",
    "    print(f\"  - {tool.name}: {tool.description[:80]}...\")\n",
    "\n",
    "# Test the tools\n",
    "print(\"\\n Testing tools:\")\n",
    "print(f\"\\n1. search_documents('embeddings'):\\n{search_documents('embeddings', k=1)[:200]}...\")\n",
    "print(f\"\\n2. get_current_time():\\n{get_current_time()}\")\n",
    "print(f\"\\n3. calculator('2 + 2'):\\n{calculator('2 + 2')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 7: Define Agent State\n",
    "\n",
    "LangGraph uses a state object that flows through the graph.\n",
    "The state maintains conversation history, intermediate results, and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Agent state structure defined\n",
      "\n",
      "ðŸ“‹ State fields:\n",
      "  - messages: Conversation history\n",
      "  - tool_calls: Tool usage tracking\n",
      "  - iterations: Loop counter\n",
      "  - final_answer: Final response\n"
     ]
    }
   ],
   "source": [
    "# Define the state structure for our agent\n",
    "# TypedDict provides type hints for the state object\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    State object that flows through the agent graph.\n",
    "    \n",
    "    Attributes:\n",
    "        messages: Conversation history (human and AI messages)\n",
    "        tool_calls: History of tool invocations\n",
    "        iterations: Number of reasoning loops completed\n",
    "        final_answer: The agent's final response\n",
    "    \"\"\"\n",
    "    # The messages list accumulates the conversation\n",
    "    # Annotated with 'add' operator to append new messages\n",
    "    messages: Annotated[List[Any], add]\n",
    "    \n",
    "    # Track tool usage for observability\n",
    "    tool_calls: Annotated[List[Dict], add]\n",
    "    \n",
    "    # Count iterations to prevent infinite loops\n",
    "    iterations: int\n",
    "    \n",
    "    # Final answer to return to the user\n",
    "    final_answer: str\n",
    "\n",
    "print(\" Agent state structure defined\")\n",
    "print(\"\\nðŸ“‹ State fields:\")\n",
    "print(\"  - messages: Conversation history\")\n",
    "print(\"  - tool_calls: Tool usage tracking\")\n",
    "print(\"  - iterations: Loop counter\")\n",
    "print(\"  - final_answer: Final response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Agent Reasoning Functions\n",
    "\n",
    "These are the core functions that define agent behavior:\n",
    "1. **agent_node**: The reasoning step - decides what to do next\n",
    "2. **tool_node**: Executes tools selected by the agent\n",
    "3. **should_continue**: Decides whether to continue or finish\n",
    "\n",
    "This implements the ReAct (Reasoning + Acting) pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Agent reasoning functions defined\n",
      "\n",
      "ðŸ”„ Agent loop logic:\n",
      "  1. agent_node: Reason about what to do\n",
      "  2. should_continue: Decide next action\n",
      "  3. tool_node: Execute tools if needed\n",
      "  4. Repeat until answer or max iterations\n"
     ]
    }
   ],
   "source": [
    "# Maximum iterations to prevent infinite loops\n",
    "MAX_ITERATIONS = 5\n",
    "\n",
    "def agent_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    The reasoning node where the agent thinks and decides what to do.\n",
    "    \n",
    "    This function:\n",
    "    1. Takes current state (conversation history)\n",
    "    2. Prompts the LLM to reason about next action\n",
    "    3. Decides whether to use a tool or provide an answer\n",
    "    4. Updates state with the decision\n",
    "    \n",
    "    Args:\n",
    "        state: Current agent state\n",
    "    \n",
    "    Returns:\n",
    "        Updated state with agent's reasoning\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state.get(\"iterations\", 0)\n",
    "    \n",
    "    # Create a prompt that explains available tools\n",
    "    tool_descriptions = \"\\n\".join([\n",
    "        f\"- {tool.name}: {tool.description}\" \n",
    "        for tool in tools\n",
    "    ])\n",
    "    \n",
    "    system_prompt = f\"\"\"You are a helpful AI assistant with access to tools. \n",
    "    \n",
    "Available tools:\n",
    "{tool_descriptions}\n",
    "\n",
    "To use a tool, respond with:\n",
    "TOOL: tool_name\n",
    "INPUT: input_for_tool\n",
    "\n",
    "If you have enough information to answer, respond with:\n",
    "ANSWER: your_final_answer\n",
    "\n",
    "Think step by step. Use tools when you need specific information.\n",
    "Current iteration: {iterations}/{MAX_ITERATIONS}\n",
    "\"\"\"\n",
    "    \n",
    "    # Get LLM response\n",
    "    # OpenAI uses chat messages format\n",
    "    full_messages = [SystemMessage(content=system_prompt)] + messages\n",
    "    response_obj = llm.invoke(full_messages)\n",
    "    response = response_obj.content  # Extract text from ChatOpenAI response\n",
    "\n",
    "    \n",
    "    # Update state with agent's response\n",
    "    state[\"messages\"].append(AIMessage(content=response))\n",
    "    state[\"iterations\"] = iterations + 1\n",
    "    \n",
    "    # Parse the response to determine next action\n",
    "    if \"ANSWER:\" in response:\n",
    "        # Extract final answer\n",
    "        answer = response.split(\"ANSWER:\")[1].strip()\n",
    "        state[\"final_answer\"] = answer\n",
    "    \n",
    "    return state\n",
    "\n",
    "def tool_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Execute tools based on agent's decision.\n",
    "    \n",
    "    This function:\n",
    "    1. Parses the agent's response for tool calls\n",
    "    2. Executes the requested tool\n",
    "    3. Adds tool results back to the conversation\n",
    "    \n",
    "    Args:\n",
    "        state: Current agent state\n",
    "    \n",
    "    Returns:\n",
    "        Updated state with tool results\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1].content\n",
    "    \n",
    "    # Parse tool call from agent's response\n",
    "    if \"TOOL:\" in last_message and \"INPUT:\" in last_message:\n",
    "        # Extract tool name and input\n",
    "        tool_name = last_message.split(\"TOOL:\")[1].split(\"\\n\")[0].strip()\n",
    "        tool_input = last_message.split(\"INPUT:\")[1].strip()\n",
    "        \n",
    "        # Find and execute the tool\n",
    "        tool = next((t for t in tools if t.name == tool_name), None)\n",
    "        \n",
    "        if tool:\n",
    "            try:\n",
    "                # Execute the tool\n",
    "                result = tool.func(tool_input)\n",
    "                \n",
    "                # Record tool usage\n",
    "                state[\"tool_calls\"].append({\n",
    "                    \"tool\": tool_name,\n",
    "                    \"input\": tool_input,\n",
    "                    \"output\": result\n",
    "                })\n",
    "                \n",
    "                # Add tool result to conversation\n",
    "                state[\"messages\"].append(\n",
    "                    HumanMessage(content=f\"Tool Result ({tool_name}):\\n{result}\")\n",
    "                )\n",
    "            except Exception as e:\n",
    "                state[\"messages\"].append(\n",
    "                    HumanMessage(content=f\"Tool Error: {str(e)}\")\n",
    "                )\n",
    "        else:\n",
    "            state[\"messages\"].append(\n",
    "                HumanMessage(content=f\"Error: Tool '{tool_name}' not found\")\n",
    "            )\n",
    "    \n",
    "    return state\n",
    "\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    Decide whether to continue the agent loop or finish.\n",
    "    \n",
    "    Stops if:\n",
    "    - Agent provided a final answer\n",
    "    - Maximum iterations reached\n",
    "    \n",
    "    Args:\n",
    "        state: Current agent state\n",
    "    \n",
    "    Returns:\n",
    "        Next node to execute: \"tools\", \"agent\", or \"end\"\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state.get(\"iterations\", 0)\n",
    "    last_message = messages[-1].content if messages else \"\"\n",
    "    \n",
    "    # Stop if we have a final answer\n",
    "    if \"ANSWER:\" in last_message or state.get(\"final_answer\"):\n",
    "        return \"end\"\n",
    "    \n",
    "    # Stop if max iterations reached\n",
    "    if iterations >= MAX_ITERATIONS:\n",
    "        state[\"final_answer\"] = \"Maximum iterations reached. Unable to complete task.\"\n",
    "        return \"end\"\n",
    "    \n",
    "    # If agent wants to use a tool, go to tool node\n",
    "    if \"TOOL:\" in last_message:\n",
    "        return \"tools\"\n",
    "    \n",
    "    # Otherwise, continue reasoning\n",
    "    return \"agent\"\n",
    "\n",
    "print(\" Agent reasoning functions defined\")\n",
    "print(\"\\nðŸ”„ Agent loop logic:\")\n",
    "print(\"  1. agent_node: Reason about what to do\")\n",
    "print(\"  2. should_continue: Decide next action\")\n",
    "print(\"  3. tool_node: Execute tools if needed\")\n",
    "print(\"  4. Repeat until answer or max iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Build the LangGraph Agent\n",
    "\n",
    "Now we assemble everything into a graph structure:\n",
    "- Nodes: agent_node, tool_node\n",
    "- Edges: Conditional routing based on should_continue\n",
    "- State: Flows through the graph and accumulates results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LangGraph agent compiled successfully!\n",
      "\n",
      " Agent graph structure:\n",
      "  START â†’ agent â†’ [should_continue] â†’ tools â†’ agent â†’ ... â†’ END\n",
      "\n",
      "ðŸ”„ Workflow:\n",
      "  1. User query enters at 'agent' node\n",
      "  2. Agent reasons and decides action\n",
      "  3. If tool needed: execute and return to agent\n",
      "  4. If answer ready: proceed to END\n",
      "  5. State persists across iterations\n"
     ]
    }
   ],
   "source": [
    "# Create the state graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "# Each node is a function that processes and updates state\n",
    "workflow.add_node(\"agent\", agent_node)  # Reasoning node\n",
    "workflow.add_node(\"tools\", tool_node)    # Tool execution node\n",
    "\n",
    "# Set the entry point - where the graph starts\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# Add conditional edges\n",
    "# After each node, should_continue decides the next step\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",           # From agent node\n",
    "    should_continue,   # Use this function to decide\n",
    "    {\n",
    "        \"tools\": \"tools\",  # If \"tools\", go to tools node\n",
    "        \"agent\": \"agent\",  # If \"agent\", loop back to agent node\n",
    "        \"end\": END         # If \"end\", finish the graph\n",
    "    }\n",
    ")\n",
    "\n",
    "# After tool execution, always go back to agent for reasoning\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Add memory for state persistence\n",
    "# This allows the agent to remember across multiple runs\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compile the graph into a runnable agent\n",
    "agent = workflow.compile(checkpointer=memory)\n",
    "\n",
    "print(\" LangGraph agent compiled successfully!\")\n",
    "print(\"\\n Agent graph structure:\")\n",
    "print(\"  START â†’ agent â†’ [should_continue] â†’ tools â†’ agent â†’ ... â†’ END\")\n",
    "print(\"\\nðŸ”„ Workflow:\")\n",
    "print(\"  1. User query enters at 'agent' node\")\n",
    "print(\"  2. Agent reasons and decides action\")\n",
    "print(\"  3. If tool needed: execute and return to agent\")\n",
    "print(\"  4. If answer ready: proceed to END\")\n",
    "print(\"  5. State persists across iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Helper Function to Run Agent\n",
    "\n",
    "Create a convenient function to interact with our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Helper function created\n",
      "\n",
      "ðŸ“ž Usage: run_agent('your question here')\n"
     ]
    }
   ],
   "source": [
    "def run_agent(query: str, thread_id: str = \"default\", verbose: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Run the agent with a user query.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question or request\n",
    "        thread_id: Conversation thread identifier for state persistence\n",
    "        verbose: Whether to print intermediate steps\n",
    "    \n",
    "    Returns:\n",
    "        Final answer from the agent\n",
    "    \"\"\"\n",
    "    # Initialize state with user query\n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=query)],\n",
    "        \"tool_calls\": [],\n",
    "        \"iterations\": 0,\n",
    "        \"final_answer\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Configuration for state persistence\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\" AGENT EXECUTION: {query}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Run the agent graph\n",
    "    result = agent.invoke(initial_state, config)\n",
    "    \n",
    "    # Display execution trace if verbose\n",
    "    if verbose:\n",
    "        print(\"\\nðŸ“‹ EXECUTION TRACE:\")\n",
    "        print(f\"Total iterations: {result['iterations']}\")\n",
    "        print(f\"Tools used: {len(result['tool_calls'])}\")\n",
    "        \n",
    "        if result['tool_calls']:\n",
    "            print(\"\\nðŸ”§ Tool Calls:\")\n",
    "            for i, call in enumerate(result['tool_calls'], 1):\n",
    "                print(f\"  {i}. {call['tool']}\")\n",
    "                print(f\"     Input: {call['input'][:100]}...\")\n",
    "                print(f\"     Output: {call['output'][:100]}...\\n\")\n",
    "    \n",
    "    # Extract and return final answer\n",
    "    final_answer = result.get(\"final_answer\", \"No answer generated\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\" FINAL ANSWER:\")\n",
    "        print(\"=\"*80)\n",
    "        print(final_answer)\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return final_answer\n",
    "\n",
    "print(\" Helper function created\")\n",
    "print(\"\\nðŸ“ž Usage: run_agent('your question here')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Test the Agent\n",
    "\n",
    "Let's test our agentic RAG system with various queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "TEST 1: Simple RAG Query\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      " AGENT EXECUTION: What is RAG and how does it work?\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-AMaYCohKJjROv8W6tXaQsmgs on tokens per min (TPM): Limit 30000, Requested 55630. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTEST 1: Simple RAG Query\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m#\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m answer1 = \u001b[43mrun_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is RAG and how does it work?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthread_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mrun_agent\u001b[39m\u001b[34m(query, thread_id, verbose)\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Run the agent graph\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m result = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Display execution trace if verbose\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1468\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[39m\n\u001b[32m   1466\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1467\u001b[39m     chunks = []\n\u001b[32m-> \u001b[39m\u001b[32m1468\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1469\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1471\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1472\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1473\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1474\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1475\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1476\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1477\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1478\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   1479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1221\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[39m\n\u001b[32m   1210\u001b[39m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   1211\u001b[39m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[32m   1212\u001b[39m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   1213\u001b[39m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   1214\u001b[39m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[32m   1215\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(\n\u001b[32m   1216\u001b[39m     input_keys=\u001b[38;5;28mself\u001b[39m.input_channels,\n\u001b[32m   1217\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   1218\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   1219\u001b[39m     manager=run_manager,\n\u001b[32m   1220\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   1227\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langgraph/pregel/runner.py:87\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, timeout, retry_policy)\u001b[39m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_futures\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langgraph/pregel/runner.py:190\u001b[39m, in \u001b[36m_panic_or_proceed\u001b[39m\u001b[34m(futs, timeout_exc_cls)\u001b[39m\n\u001b[32m    188\u001b[39m             inflight.pop().cancel()\n\u001b[32m    189\u001b[39m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[32m    192\u001b[39m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[32m    194\u001b[39m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langgraph/pregel/executor.py:59\u001b[39m, in \u001b[36mBackgroundExecutor.done\u001b[39m\u001b[34m(self, task)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: concurrent.futures.Future) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m         \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m GraphInterrupt:\n\u001b[32m     61\u001b[39m         \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[32m     62\u001b[39m         \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[32m     63\u001b[39m         \u001b[38;5;28mself\u001b[39m.tasks.pop(task)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langgraph/pregel/retry.py:26\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy)\u001b[39m\n\u001b[32m     24\u001b[39m task.writes.clear()\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langgraph/utils/runnable.py:341\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    339\u001b[39m context.run(_set_config_context, config)\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    343\u001b[39m     \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langgraph/utils/runnable.py:129\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m     context.run(_set_config_context, config)\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     ret = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse:\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36magent_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Get LLM response\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# OpenAI uses chat messages format\u001b[39;00m\n\u001b[32m     47\u001b[39m full_messages = [SystemMessage(content=system_prompt)] + messages\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m response_obj = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_messages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m response = response_obj.content  \u001b[38;5;66;03m# Extract text from ChatOpenAI response\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Update state with agent's response\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:372\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> BaseMessage:\n\u001b[32m    369\u001b[39m     config = ensure_config(config)\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    382\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:995\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    993\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m995\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1144\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1145\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1146\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/shipping-doc-analyst/.venv/lib/python3.11/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-AMaYCohKJjROv8W6tXaQsmgs on tokens per min (TPM): Limit 30000, Requested 55630. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "# Test 1: Simple RAG query\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"TEST 1: Simple RAG Query\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "answer1 = run_agent(\n",
    "    \"What is RAG and how does it work?\",\n",
    "    thread_id=\"test1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "TEST 2: Multi-Step Query\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      " AGENT EXECUTION: Explain LangGraph and tell me what time it is right now.\n",
      "================================================================================\n",
      "\n",
      "\n",
      "ðŸ“‹ EXECUTION TRACE:\n",
      "Total iterations: 2\n",
      "Tools used: 4\n",
      "\n",
      "ðŸ”§ Tool Calls:\n",
      "  1. search_documents\n",
      "     Input: LangGraph\n",
      "\n",
      "Now, I'll get the current time.\n",
      "\n",
      "TOOL: get_current_time...\n",
      "     Output: Result 1:\n",
      "Source: langgraph_guide.txt\n",
      "Topic: LangGraph\n",
      "Content: LangGraph is a framework for buildin...\n",
      "\n",
      "  2. search_documents\n",
      "     Input: LangGraph\n",
      "\n",
      "Now, I'll get the current time.\n",
      "\n",
      "TOOL: get_current_time...\n",
      "     Output: Result 1:\n",
      "Source: langgraph_guide.txt\n",
      "Topic: LangGraph\n",
      "Content: LangGraph is a framework for buildin...\n",
      "\n",
      "  3. search_documents\n",
      "     Input: LangGraph\n",
      "\n",
      "Now, I'll get the current time.\n",
      "\n",
      "TOOL: get_current_time...\n",
      "     Output: Result 1:\n",
      "Source: langgraph_guide.txt\n",
      "Topic: LangGraph\n",
      "Content: LangGraph is a framework for buildin...\n",
      "\n",
      "  4. search_documents\n",
      "     Input: LangGraph\n",
      "\n",
      "Now, I'll get the current time.\n",
      "\n",
      "TOOL: get_current_time...\n",
      "     Output: Result 1:\n",
      "Source: langgraph_guide.txt\n",
      "Topic: LangGraph\n",
      "Content: LangGraph is a framework for buildin...\n",
      "\n",
      "\n",
      "================================================================================\n",
      " FINAL ANSWER:\n",
      "================================================================================\n",
      "LangGraph is a framework designed for building stateful, multi-actor applications that utilize Large Language Models (LLMs). It enhances LangChain by providing a graph-based orchestration system where nodes represent computation steps and edges define the transitions between these steps. Key features of LangGraph include managing state across nodes, enabling conditional branching, supporting cycles for iterative processing, implementing human-in-the-loop patterns, and offering checkpointing for persistence. It is particularly useful for creating complex agentic workflows.\n",
      "\n",
      "Now, let me tell you the current time.\n",
      "\n",
      "TOOL: get_current_time\n",
      "INPUT: None\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Multi-step query requiring tool use\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"TEST 2: Multi-Step Query\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "answer2 = run_agent(\n",
    "    \"Explain LangGraph and tell me what time it is right now.\",\n",
    "    thread_id=\"test2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Query requiring multiple document lookups\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"TEST 3: Multiple Document Lookups\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "answer3 = run_agent(\n",
    "    \"Compare FAISS and embeddings. How do they work together?\",\n",
    "    thread_id=\"test3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Query combining RAG and calculation\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"TEST 4: RAG + Calculation\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "answer4 = run_agent(\n",
    "    \"If I have 7 documents about AI agents, and I split each into 3 chunks, how many chunks total? Also, what are AI agents?\",\n",
    "    thread_id=\"test4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Interactive Agent Session\n",
    "\n",
    "Create an interactive chat interface for continuous conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_agent_session():\n",
    "    \"\"\"\n",
    "    Start an interactive session with the agent.\n",
    "    Type 'quit' or 'exit' to end the session.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" INTERACTIVE AGENT SESSION\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nWelcome! I'm your AI assistant with access to:\")\n",
    "    print(\"  - Document search (AI/ML knowledge base)\")\n",
    "    print(\"  - Current time lookup\")\n",
    "    print(\"  - Basic calculator\")\n",
    "    print(\"\\nType 'quit' or 'exit' to end the session.\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    thread_id = f\"interactive_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            user_input = input(\"\\nðŸ‘¤ You: \").strip()\n",
    "            \n",
    "            # Check for exit commands\n",
    "            if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "                print(\"\\n Goodbye! Thanks for chatting.\\n\")\n",
    "                break\n",
    "            \n",
    "            # Skip empty inputs\n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            # Run the agent\n",
    "            answer = run_agent(user_input, thread_id=thread_id, verbose=False)\n",
    "            print(f\"\\n Agent: {answer}\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n Session interrupted. Goodbye!\\n\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n Error: {str(e)}\")\n",
    "            print(\"Please try again.\\n\")\n",
    "\n",
    "# Uncomment to start interactive session\n",
    "# interactive_agent_session()\n",
    "\n",
    "print(\"\\n Interactive session function ready\")\n",
    "print(\"\\n To start chatting, uncomment and run:\")\n",
    "print(\"   interactive_agent_session()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: MCP Server Integration (Conceptual)\n",
    "\n",
    "While full MCP implementation requires a running server, here's how to integrate MCP tools.\n",
    "This demonstrates the pattern for production deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP Tool Integration Pattern\n",
    "# In production, you would connect to actual MCP servers\n",
    "\n",
    "class MCPToolWrapper:\n",
    "    \"\"\"\n",
    "    Wrapper for MCP (Model Context Protocol) tools.\n",
    "    \n",
    "    MCP provides a standardized way to expose tools to AI agents.\n",
    "    This wrapper demonstrates how to integrate MCP tools into our agent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tool_name: str, tool_description: str, endpoint: str):\n",
    "        \"\"\"\n",
    "        Initialize MCP tool wrapper.\n",
    "        \n",
    "        Args:\n",
    "            tool_name: Name of the tool\n",
    "            tool_description: Description for the agent\n",
    "            endpoint: MCP server endpoint URL\n",
    "        \"\"\"\n",
    "        self.name = tool_name\n",
    "        self.description = tool_description\n",
    "        self.endpoint = endpoint\n",
    "    \n",
    "    def call(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Call the MCP tool via HTTP/JSON-RPC.\n",
    "        \n",
    "        In production, this would make actual HTTP requests to MCP servers.\n",
    "        \n",
    "        Args:\n",
    "            input_data: Tool input parameters\n",
    "        \n",
    "        Returns:\n",
    "            Tool execution result\n",
    "        \"\"\"\n",
    "        # Simulated MCP call (in production, use requests library)\n",
    "        print(f\"ðŸ“¡ [MCP] Calling {self.name} at {self.endpoint}\")\n",
    "        print(f\"ðŸ“¡ [MCP] Input: {input_data}\")\n",
    "        \n",
    "        # Example: actual implementation would look like:\n",
    "        # import requests\n",
    "        # response = requests.post(\n",
    "        #     self.endpoint,\n",
    "        #     json={\n",
    "        #         \"jsonrpc\": \"2.0\",\n",
    "        #         \"method\": \"tools/call\",\n",
    "        #         \"params\": {\n",
    "        #             \"name\": self.name,\n",
    "        #             \"arguments\": input_data\n",
    "        #         },\n",
    "        #         \"id\": 1\n",
    "        #     }\n",
    "        # )\n",
    "        # return response.json()[\"result\"]\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"MCP tool executed (simulated)\"\n",
    "        }\n",
    "\n",
    "# Example MCP tool definitions\n",
    "mcp_tools_config = [\n",
    "    {\n",
    "        \"name\": \"filesystem_read\",\n",
    "        \"description\": \"Read contents of a file from the filesystem\",\n",
    "        \"endpoint\": \"http://localhost:3000/mcp/filesystem\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"database_query\",\n",
    "        \"description\": \"Execute SQL queries on connected databases\",\n",
    "        \"endpoint\": \"http://localhost:3000/mcp/database\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"web_scrape\",\n",
    "        \"description\": \"Scrape content from web pages\",\n",
    "        \"endpoint\": \"http://localhost:3000/mcp/web\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create MCP tool wrappers\n",
    "mcp_tools = [\n",
    "    MCPToolWrapper(\n",
    "        tool_name=config[\"name\"],\n",
    "        tool_description=config[\"description\"],\n",
    "        endpoint=config[\"endpoint\"]\n",
    "    )\n",
    "    for config in mcp_tools_config\n",
    "]\n",
    "\n",
    "print(\" MCP tool integration pattern demonstrated\")\n",
    "print(\"\\nðŸ“¡ MCP Tools (conceptual):\")\n",
    "for tool in mcp_tools:\n",
    "    print(f\"  - {tool.name}: {tool.description}\")\n",
    "    print(f\"    Endpoint: {tool.endpoint}\")\n",
    "\n",
    "print(\"\\n To use in production:\")\n",
    "print(\"  1. Start MCP server: mcp-server --port 3000\")\n",
    "print(\"  2. Implement actual HTTP calls in MCPToolWrapper.call()\")\n",
    "print(\"  3. Add MCP tools to the agent's tools list\")\n",
    "print(\"  4. Agent can now use MCP tools like any other tool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 14: Agent Analytics and Observability\n",
    "\n",
    "Monitor and analyze agent performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_agent_performance(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze agent execution performance.\n",
    "    \n",
    "    Args:\n",
    "        state: Final agent state after execution\n",
    "    \n",
    "    Returns:\n",
    "        Performance metrics dictionary\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"total_iterations\": state.get(\"iterations\", 0),\n",
    "        \"total_messages\": len(state.get(\"messages\", [])),\n",
    "        \"tool_calls_count\": len(state.get(\"tool_calls\", [])),\n",
    "        \"tools_used\": [],\n",
    "        \"success\": bool(state.get(\"final_answer\")),\n",
    "        \"answer_length\": len(state.get(\"final_answer\", \"\"))\n",
    "    }\n",
    "    \n",
    "    # Analyze tool usage\n",
    "    tool_usage = {}\n",
    "    for call in state.get(\"tool_calls\", []):\n",
    "        tool_name = call[\"tool\"]\n",
    "        tool_usage[tool_name] = tool_usage.get(tool_name, 0) + 1\n",
    "    \n",
    "    metrics[\"tools_used\"] = tool_usage\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Example: Run agent and analyze performance\n",
    "print(\"\\n PERFORMANCE ANALYSIS EXAMPLE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Run a test query\n",
    "test_state = {\n",
    "    \"messages\": [HumanMessage(content=\"What are embeddings?\")],\n",
    "    \"tool_calls\": [],\n",
    "    \"iterations\": 0,\n",
    "    \"final_answer\": \"\"\n",
    "}\n",
    "\n",
    "result_state = agent.invoke(test_state, {\"configurable\": {\"thread_id\": \"analysis\"}})\n",
    "metrics = analyze_agent_performance(result_state)\n",
    "\n",
    "print(\"ðŸ“ˆ Performance Metrics:\")\n",
    "print(f\"  - Total Iterations: {metrics['total_iterations']}\")\n",
    "print(f\"  - Messages Exchanged: {metrics['total_messages']}\")\n",
    "print(f\"  - Tool Calls: {metrics['tool_calls_count']}\")\n",
    "print(f\"  - Tools Used: {metrics['tools_used']}\")\n",
    "print(f\"  - Success: {metrics['success']}\")\n",
    "print(f\"  - Answer Length: {metrics['answer_length']} characters\")\n",
    "\n",
    "print(\"\\n Analytics system ready for production monitoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 15: Save and Load Vector Store\n",
    "\n",
    "Persist your vector store for reuse across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vector store to disk\n",
    "def save_vectorstore(vectorstore, path: str = \"faiss_index\"):\n",
    "    \"\"\"\n",
    "    Save FAISS vector store to disk.\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: FAISS vectorstore instance\n",
    "        path: Directory path to save to\n",
    "    \"\"\"\n",
    "    vectorstore.save_local(path)\n",
    "    print(f\" Vector store saved to '{path}'\")\n",
    "\n",
    "# Load the vector store from disk\n",
    "def load_vectorstore(path: str = \"faiss_index\", embeddings_model=None):\n",
    "    \"\"\"\n",
    "    Load FAISS vector store from disk.\n",
    "    \n",
    "    Args:\n",
    "        path: Directory path to load from\n",
    "        embeddings_model: Embeddings model (must match the one used to create)\n",
    "    \n",
    "    Returns:\n",
    "        Loaded FAISS vectorstore\n",
    "    \"\"\"\n",
    "    if embeddings_model is None:\n",
    "        embeddings_model = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "    \n",
    "    vectorstore = FAISS.load_local(\n",
    "        path, \n",
    "        embeddings_model,\n",
    "        allow_dangerous_deserialization=True  # Required for pickle loading\n",
    "    )\n",
    "    print(f\" Vector store loaded from '{path}'\")\n",
    "    return vectorstore\n",
    "\n",
    "# Save current vector store\n",
    "save_vectorstore(vectorstore, \"faiss_index\")\n",
    "\n",
    "# Example: Load it back\n",
    "# loaded_vectorstore = load_vectorstore(\"faiss_index\", embeddings)\n",
    "\n",
    "print(\"\\n Usage:\")\n",
    "print(\"  - Save: save_vectorstore(vectorstore, 'path')\")\n",
    "print(\"  - Load: loaded_vs = load_vectorstore('path', embeddings)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Summary and Next Steps\n",
    "\n",
    "### What We Built:\n",
    "\n",
    "1.  **Flexible LLM**: OpenAI GPT-4 or Local Ollama (Llama 3.2)\n",
    "2.  **Flexible Embeddings**: OpenAI or HuggingFace sentence-transformers\n",
    "3.  **FAISS Vector Database**: Industry-standard, fast, scalable semantic search\n",
    "4.  **RAG System**: Document retrieval and augmented generation\n",
    "5.  **Agentic Framework**: LangGraph with state management\n",
    "6.  **Tool Integration**: Search, calculator, time tools\n",
    "7.  **MCP Pattern**: Model Context Protocol integration pattern\n",
    "8.  **PyMuPDF**: Fast, reliable PDF processing (3-5x faster than pypdf)\n",
    "9.  **Observability**: Performance monitoring and analytics\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- **Flexible**: Use OpenAI for best quality or Ollama for free local operation\n",
    "- **Fast**: FAISS provides lightning-fast vector search\n",
    "- **Reliable**: PyMuPDF handles complex PDFs better than alternatives\n",
    "- **Privacy Options**: Local option keeps all data on your machine\n",
    "- **Extensible**: Easy to add new tools and documents\n",
    "- **Production-Ready**: State management, error handling, monitoring\n",
    "- **Open Source**: All components are FOSS\n",
    "\n",
    "### Technology Choices:\n",
    "\n",
    "**FAISS vs Other Vector Databases:**\n",
    "-  10-100x faster than basic implementations\n",
    "-  Memory efficient for large datasets\n",
    "-  Industry battle-tested (Facebook, Google use it)\n",
    "-  Multiple index types for different scales\n",
    "-  No external server required (unlike Chroma, Pinecone)\n",
    "-  Perfect for local development and production\n",
    "\n",
    "**PyMuPDF vs pypdf:**\n",
    "-  3-5x faster processing\n",
    "-  Better text extraction quality\n",
    "-  Handles complex PDFs (forms, tables, images)\n",
    "-  More reliable with large documents\n",
    "-  Active development and maintenance\n",
    "\n",
    "### LLM Comparison:\n",
    "\n",
    "**OpenAI GPT-4:**\n",
    "-  Best quality responses\n",
    "-  Faster inference\n",
    "-  Larger context windows\n",
    "-  Better reasoning capabilities\n",
    "-  Requires API key and costs per token\n",
    "\n",
    "**Local Ollama:**\n",
    "-  Completely free\n",
    "-  Full privacy (no data sent externally)\n",
    "-  No API limits\n",
    "-  Works offline\n",
    "-  Slower inference\n",
    "-  Requires good hardware\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Add More Documents**: Load PDFs with PyMuPDF, websites, or databases\n",
    "2. **Custom Tools**: Create domain-specific tools\n",
    "3. **Scale Up**: Use FAISS IVF or HNSW indices for millions of vectors\n",
    "4. **Multi-Agent**: Build specialized agent teams\n",
    "5. **MCP Server**: Deploy actual MCP servers\n",
    "6. **Web Interface**: Add Gradio or Streamlit UI\n",
    "7. **Fine-Tuning**: Train models on your data\n",
    "8. **Production Deploy**: Containerize and deploy with FAISS persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick reference: Main components\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" QUICK REFERENCE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"1ï¸âƒ£  LLM Setup:\")\n",
    "\n",
    "print(\"    # OpenAI\")\n",
    "print(\"    llm = ChatOpenAI(model='gpt-4o', api_key=os.getenv('OPENAI_API_KEY'))\")\n",
    "print(\"    embeddings = OpenAIEmbeddings()\")\n",
    "\n",
    "\n",
    "print(\"\\n2ï¸âƒ£  Load PDFs (PyMuPDF):\")\n",
    "print(\"    documents = load_pdf_with_pymupdf('your_file.pdf')\")\n",
    "print(\"    # 3-5x faster than pypdf!\")\n",
    "\n",
    "print(\"\\n3ï¸âƒ£  FAISS Vector Store:\")\n",
    "print(\"    vectorstore = FAISS.from_documents(docs, embeddings)\")\n",
    "print(\"    # Lightning-fast similarity search\")\n",
    "\n",
    "print(\"\\n4ï¸âƒ£  Search:\")\n",
    "print(\"    results = vectorstore.similarity_search(query, k=3)\")\n",
    "\n",
    "print(\"\\n5ï¸âƒ£  Run Agent:\")\n",
    "print(\"    answer = run_agent('your question')\")\n",
    "\n",
    "print(\"\\n6ï¸âƒ£  Interactive:\")\n",
    "print(\"    interactive_agent_session()\")\n",
    "\n",
    "print(\"\\n7ï¸âƒ£  Save/Load FAISS:\")\n",
    "print(\"    vectorstore.save_local('faiss_index')\")\n",
    "print(\"    vectorstore = FAISS.load_local('faiss_index', embeddings)\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "provider = \"OpenAI GPT-4\"\n",
    "print(f\" All systems ready! Using {provider}\")\n",
    "print(\" Vector Database: FAISS (Industry Standard)\")\n",
    "print(\" PDF Processor: PyMuPDF (Fast & Reliable)\")\n",
    "print(\"\\nðŸŽ‰ Congratulations on building a complete AI agent!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
